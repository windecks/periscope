diff -ruN linux-5.15.0/arch/arm64/include/asm/debug-monitors.h linux-5.15.0-patched/arch/arm64/include/asm/debug-monitors.h
--- linux-5.15.0/arch/arm64/include/asm/debug-monitors.h	2025-09-17 10:46:55.000000000 -0400
+++ linux-5.15.0-patched/arch/arm64/include/asm/debug-monitors.h	2025-09-17 10:51:24.113130075 -0400
@@ -51,6 +51,11 @@
 
 #define CACHE_FLUSH_IS_SAFE		1
 
+/* hwiotrace BRK opcodes with ESR encoding */
+#define BRK64_ESR_HWIO_MASK     0xFFFF
+#define BRK64_ESR_HWIO_PROBES   0x0005
+#define BRK64_OPCODE_HWIO_PROBES (AARCH64_BREAK_MON | (BRK64_ESR_HWIO_PROBE << 5))
+
 /* kprobes BRK opcodes with ESR encoding  */
 #define BRK64_OPCODE_KPROBES	(AARCH64_BREAK_MON | (KPROBES_BRK_IMM << 5))
 #define BRK64_OPCODE_KPROBES_SS	(AARCH64_BREAK_MON | (KPROBES_BRK_SS_IMM << 5))
diff -ruN linux-5.15.0/arch/arm64/Kconfig linux-5.15.0-patched/arch/arm64/Kconfig
--- linux-5.15.0/arch/arm64/Kconfig	2025-09-17 10:46:55.000000000 -0400
+++ linux-5.15.0-patched/arch/arm64/Kconfig	2025-09-17 11:21:59.913185518 -0400
@@ -354,6 +354,25 @@
 
 menu "Kernel Features"
 
+config KHWIO_STREAMING_DMA
+	depends on HWIOTRACE
+	bool "trace streaming dma"
+	help
+	  Support streaming dma tracing
+
+config KHWIO_CONSISTENT_DMA
+	depends on HWIOTRACE
+	bool "trace consistent dma"
+	help
+	  Support consistent dma tracing
+
+config KHWIO_MMIO
+	depends on HWIOTRACE
+	bool "trace mmio"
+	help
+	  Support memory mapped io tracing
+
+
 menu "ARM errata workarounds via the alternatives framework"
 
 config ARM64_WORKAROUND_CLEAN_CACHE
diff -ruN linux-5.15.0/arch/arm64/kernel/debug-monitors.c linux-5.15.0-patched/arch/arm64/kernel/debug-monitors.c
--- linux-5.15.0/arch/arm64/kernel/debug-monitors.c	2025-09-17 10:46:55.000000000 -0400
+++ linux-5.15.0-patched/arch/arm64/kernel/debug-monitors.c	2025-09-17 11:11:21.302571829 -0400
@@ -10,6 +10,7 @@
 #include <linux/cpu.h>
 #include <linux/debugfs.h>
 #include <linux/hardirq.h>
+#include <linux/hwiotrace.h>
 #include <linux/init.h>
 #include <linux/ptrace.h>
 #include <linux/kprobes.h>
@@ -263,7 +264,13 @@
 		 * to the active-not-pending state).
 		 */
 		user_rewind_single_step(current);
-	} else if (!handler_found) {
+	}
+#ifdef CONFIG_HWIOTRACE
+    else if (khwio_single_step_handler(regs, esr) == DBG_HOOK_HANDLED) {
+        return 0;
+    }
+#endif
+    else if (!handler_found) {
 		pr_warn("Unexpected kernel single-step exception at EL1\n");
 		/*
 		 * Re-enable stepping since we know that we will be
@@ -330,7 +337,14 @@
 
 	if (user_mode(regs)) {
 		send_user_sigtrap(TRAP_BRKPT);
-	} else {
+    }
+#ifdef CONFIG_HWIOTRACE
+    else if ((esr & BRK64_ESR_HWIO_MASK) == BRK64_ESR_HWIO_PROBES) {
+        if (khwio_breakpoint_handler(regs, esr) != DBG_HOOK_HANDLED)
+            return -EFAULT;
+    }
+#endif
+	else {
 		pr_warn("Unexpected kernel BRK exception at EL1\n");
 		return -EFAULT;
 	}
diff -ruN linux-5.15.0/arch/arm64/mm/decode.c linux-5.15.0-patched/arch/arm64/mm/decode.c
--- linux-5.15.0/arch/arm64/mm/decode.c	1969-12-31 19:00:00.000000000 -0500
+++ linux-5.15.0-patched/arch/arm64/mm/decode.c	2025-09-17 11:20:52.269306477 -0400
@@ -0,0 +1,800 @@
+#ifdef __KERNEL__
+#include <linux/printk.h>
+#include <linux/errno.h>
+#include <asm/page.h>
+#include <linux/decode.h>
+#else
+#include "decode.h"
+#endif
+
+static long long get_reg_val(struct pt_regs *regs, int reg_idx, bool *valid)
+{
+	*valid = false;
+	if (reg_idx == 31) {
+		*valid = true;
+		return 0;
+	} else if (reg_idx >= 0 && reg_idx < 31) {
+		*valid = true;
+		return regs->regs[reg_idx];
+	}
+	return -1;
+}
+
+// we need to use one byte copies
+// otherwise we will get alignment faults because
+// remapped mem is not "normal"
+static void update_mem(long long reg_data, char *data_ptr_c, char *data_ptr_c2,
+		       unsigned int len, char *transfer_val)
+{
+	size_t i;
+	char *reg_data_ptr = (char *)&reg_data;
+	unsigned int bytes_total = len;
+	size_t transfer_val_idx = 0;
+	if (unlikely(data_ptr_c2)) {
+		unsigned int bytes_left =
+			PAGE_SIZE - (((unsigned long)data_ptr_c) & 0xfff);
+		if (unlikely(bytes_left <= bytes_total)) {
+			pr_info("Splitting mem access %d = %d + %d\n",
+				bytes_total, bytes_left,
+				bytes_total - bytes_left);
+			for (i = 0; i < bytes_left; ++i) {
+				data_ptr_c[i] = reg_data_ptr[i];
+				transfer_val[transfer_val_idx++] =
+					reg_data_ptr[i];
+			}
+			for (i = 0; i < (bytes_total - bytes_left); ++i) {
+				data_ptr_c2[i] = reg_data_ptr[bytes_left + i];
+				transfer_val[transfer_val_idx++] =
+					reg_data_ptr[bytes_left + i];
+			}
+			return;
+		}
+	}
+	for (i = 0; i < bytes_total; ++i) {
+		data_ptr_c[i] = reg_data_ptr[i];
+		transfer_val[transfer_val_idx++] = reg_data_ptr[i];
+	}
+}
+
+// Unused
+#if 0
+static void get_mem(long long* mem_data, char* data_ptr_c, char* data_ptr_c2, unsigned int len)
+{
+   size_t i;
+   char* mem_data_ptr;
+   unsigned int bytes_total;
+   *mem_data = 0;
+   mem_data_ptr = (char*)mem_data;
+   bytes_total = len;
+   if(unlikely(data_ptr_c2)) {
+      unsigned int bytes_left = PAGE_SIZE - (((unsigned long)data_ptr_c) & 0xfff);
+      if(unlikely(bytes_left <= bytes_total)) {
+         pr_info("Splitting mem access %d = %d + %d\n",
+               bytes_total, bytes_left, bytes_total - bytes_left);
+         for(i = 0; i < bytes_left; ++i) {
+            mem_data_ptr[i] = data_ptr_c[i];
+         }
+         for(i = 0; i < (bytes_total - bytes_left); ++i) {
+            mem_data_ptr[bytes_left + i] = data_ptr_c2[i];
+         }
+         return;
+      }
+   }
+   for(i = 0; i < bytes_total; ++i) {
+      mem_data_ptr[i] = mem_data[i];
+   }
+}
+#endif
+
+int emulate_st(struct Insn *instr, struct pt_regs *regs, char *data_ptr_c,
+	       char *data_ptr_c2)
+{
+	bool reg_valid = false;
+	long long reg_data = get_reg_val(regs, instr->rt, &reg_valid);
+	instr->transfer_val[0] = 0;
+	instr->transfer_val[1] = 0;
+
+	update_mem(reg_data, data_ptr_c, data_ptr_c2, instr->len,
+		   (char *)instr->transfer_val);
+	if (data_ptr_c2 == NULL) {
+		pr_info("emulate_st (%d) reg[%d] %llx -> %llx\n", instr->len,
+			instr->rt, reg_data, *(long long *)data_ptr_c);
+	} else {
+		pr_info("emulate_st (%d) reg[%d] %llx\n", instr->len, instr->rt,
+			reg_data);
+	}
+
+	if (!reg_valid) {
+		pr_err("emulate_st error getting register rt[%u] = %llx\n",
+		       instr->rt, reg_data);
+		return -EFAULT;
+	}
+
+	reg_data = get_reg_val(regs, instr->rt2, &reg_valid);
+	if (reg_valid) {
+		unsigned int bytes_total;
+		unsigned int bytes_left;
+		if (instr->len != 4 && instr->len != 8) {
+			pr_err("emulate_st error invalid instruction length for stp %u\n",
+			       instr->len);
+			return -EFAULT;
+		}
+
+		bytes_total = instr->len;
+		bytes_left = PAGE_SIZE - (((unsigned long)data_ptr_c) & 0xfff);
+		if (unlikely(bytes_left <= bytes_total)) {
+			pr_info("switching page on stp left %d, total %d\n",
+				bytes_left, bytes_total);
+			data_ptr_c = data_ptr_c2;
+		} else {
+			data_ptr_c += bytes_total;
+		}
+
+		update_mem(reg_data, data_ptr_c, NULL, instr->len,
+			   (char *)instr->transfer_val);
+
+		pr_info("emulate_st (%d) reg[%d] %llx -> %llx\n", instr->len,
+			instr->rt2, regs->regs[instr->rt2],
+			*(long long *)data_ptr_c);
+		if (instr->wback) {
+			regs->regs[instr->rn] =
+				regs->regs[instr->rn] + instr->len;
+		}
+	}
+	if (instr->rs >= 0) {
+		regs->regs[instr->rs] = 1;
+	}
+
+	if (instr->wback) {
+		regs->regs[instr->rn] = regs->regs[instr->rn] + instr->len;
+	}
+
+	return 0;
+}
+
+int update_reg(long long *reg_ptr_c, char *data_ptr_c, struct Insn *instr)
+{
+	//instr->sign_extend = true;
+	switch (instr->len) {
+	case 1:
+		if (instr->sign_extend) {
+			*reg_ptr_c = *(char *)data_ptr_c;
+			instr->transfer_val[0] = *data_ptr_c;
+		} else {
+			*reg_ptr_c = *(unsigned char *)data_ptr_c;
+			instr->transfer_val[0] = *data_ptr_c;
+		}
+		break;
+	case 2:
+		if (instr->sign_extend) {
+			*reg_ptr_c = *(short *)data_ptr_c;
+			instr->transfer_val[0] = *(short *)data_ptr_c;
+		} else {
+			*reg_ptr_c = *(unsigned short *)data_ptr_c;
+			instr->transfer_val[0] = *(unsigned short *)data_ptr_c;
+		}
+		break;
+	case 4:
+		if (instr->sign_extend) {
+			*reg_ptr_c = *(int *)data_ptr_c;
+			instr->transfer_val[0] = *(int *)data_ptr_c;
+		} else {
+			*reg_ptr_c = *(unsigned int *)data_ptr_c;
+			instr->transfer_val[0] = *(unsigned int *)data_ptr_c;
+		}
+		break;
+	case 8:
+		if (instr->sign_extend) {
+			*reg_ptr_c = *(long long *)data_ptr_c;
+			instr->transfer_val[0] = *(long long *)data_ptr_c;
+		} else {
+			*reg_ptr_c = *(unsigned long long *)data_ptr_c;
+			instr->transfer_val[0] =
+				*(unsigned long long *)data_ptr_c;
+		}
+		break;
+	}
+	return 0;
+}
+
+int emulate_ld(struct Insn *instr, struct pt_regs *regs, char *data_ptr_c,
+	       char *data_ptr_c2)
+{
+	instr->transfer_val[0] = 0;
+	instr->transfer_val[1] = 0;
+
+	update_reg((long long *)&regs->regs[instr->rt], data_ptr_c, instr);
+	if (data_ptr_c2 == NULL)
+		pr_info("emulate_ld (%u) regs[%u] %llx <- %llx\n", instr->len,
+			instr->rt, regs->regs[instr->rt],
+			*(long long *)data_ptr_c);
+	else
+		pr_info("emulate_ld (%u) regs[%u] %llx\n", instr->len,
+			instr->rt, regs->regs[instr->rt]);
+
+	// handle ldp x1, x2, [mem]
+	// ldp and stp only work on 32 or 64 bit
+	if (instr->rt2 >= 0 && instr->rt2 < 31) {
+		unsigned int bytes_total;
+		unsigned int bytes_left;
+		if (instr->len != 4 && instr->len != 8) {
+			pr_err("emulate_ld error invalid instruction length for ldp %u\n",
+			       instr->len);
+			return -EFAULT;
+		}
+
+		// handle page boundary
+		bytes_total = instr->len;
+		bytes_left = PAGE_SIZE - (((unsigned long)data_ptr_c) & 0xfff);
+		if (unlikely(bytes_left <= bytes_total)) {
+			pr_info("switching page on stp left %d, total %d\n",
+				bytes_left, bytes_total);
+			data_ptr_c = data_ptr_c2;
+		} else {
+			data_ptr_c += bytes_total;
+		}
+
+		update_reg((long long *)&regs->regs[instr->rt2], data_ptr_c,
+			   instr);
+
+		if (instr->wback) {
+			regs->regs[instr->rn] =
+				regs->regs[instr->rn] + instr->len;
+		}
+	}
+
+	if (instr->wback) {
+		regs->regs[instr->rn] = regs->regs[instr->rn] + instr->len;
+	}
+
+	return 0;
+}
+
+long long sd_field(unsigned long long insn, unsigned int start,
+		   unsigned int len)
+{
+	unsigned long long value;
+	if (len == 64)
+		return insn;
+	value = insn >> start;
+	return value & ~(-1ULL << len);
+}
+
+static int LdSt_P_Signed(uint32_t insn, struct Insn *out)
+{
+	long long size = sd_field(insn, 30, 2);
+	long long o;
+
+	if (size == 0) {
+		out->len = 4;
+		out->sign_extend = false;
+	} else if (size == 2) {
+		out->len = 8;
+		out->sign_extend = false;
+	} else {
+		pr_err("Error decoding %x\n", insn);
+		return -EFAULT;
+	}
+
+	o = sd_field(insn, 30, 1);
+	if (o == 0) {
+		out->sign_extend = false;
+	} else {
+		out->sign_extend = true;
+	}
+
+	out->rt = sd_field(insn, 0, 5);
+	out->rn = sd_field(insn, 5, 5);
+	out->rt2 = sd_field(insn, 10, 5);
+	out->rm = 0;
+	out->of = sd_field(insn, 15, 7);
+	out->ld = sd_field(insn, 22, 1);
+	out->p = 0;
+	out->wback = false;
+	return 0;
+}
+
+static int LdSt_P_UnSigned(uint32_t insn, struct Insn *out)
+{
+	long long size = sd_field(insn, 30, 2);
+	long long o;
+	if (size == 0) {
+		out->len = 4;
+		out->sign_extend = false;
+	} else if (size == 2) {
+		out->len = 8;
+		out->sign_extend = false;
+	} else {
+		pr_err("Error decoding %x\n", insn);
+		return -EFAULT;
+	}
+
+	o = sd_field(insn, 30, 1);
+	if (o == 0) {
+		out->sign_extend = false;
+	} else {
+		out->sign_extend = true;
+	}
+
+	out->rt = sd_field(insn, 0, 5);
+	out->rn = sd_field(insn, 5, 5);
+	out->rt2 = sd_field(insn, 10, 5);
+	out->rm = 0;
+	out->of = sd_field(insn, 15, 7);
+	out->ld = sd_field(insn, 22, 1);
+	out->p = 0;
+	out->wback = true;
+	return 0;
+}
+
+static int LdSt_B_Imm_Signed(uint32_t insn, struct Insn *out)
+{
+	long long o = sd_field(insn, 23, 1);
+	long long owb;
+	if (o == 0) {
+		out->sign_extend = false;
+	} else {
+		out->sign_extend = true;
+	}
+
+	out->len = 1;
+	out->rt = sd_field(insn, 0, 5);
+	out->rn = sd_field(insn, 5, 5);
+	out->rm = 0;
+	out->of = sd_field(insn, 12, 9);
+	out->ld = sd_field(insn, 22, 1);
+	out->p = 0;
+
+	owb = sd_field(insn, 10, 2);
+	if (owb == 0)
+		out->wback = false;
+	else
+		out->wback = true;
+	return 0;
+}
+
+static int LdSt_B_Imm_UnSigned(uint32_t insn, struct Insn *out)
+{
+	long long o = sd_field(insn, 23, 1);
+	if (o == 0) {
+		out->sign_extend = false;
+	} else {
+		out->sign_extend = true;
+	}
+
+	out->len = 1;
+	out->rt = sd_field(insn, 0, 5);
+	out->rn = sd_field(insn, 5, 5);
+	out->rm = 0;
+	out->of = sd_field(insn, 10, 12);
+	out->ld = sd_field(insn, 22, 1);
+	out->p = 0;
+	out->wback = false;
+	return 0;
+}
+
+static int LdSt_B_Reg(uint32_t insn, struct Insn *out)
+{
+	long long o = sd_field(insn, 23, 1);
+	if (o == 0) {
+		out->sign_extend = false;
+	} else {
+		out->sign_extend = true;
+	}
+
+	out->len = 1;
+	out->rt = sd_field(insn, 0, 5);
+	out->rn = sd_field(insn, 5, 5);
+	out->rm = sd_field(insn, 16, 5);
+	out->of = 0;
+	out->ld = sd_field(insn, 22, 1);
+	out->p = 0;
+	out->wback = false;
+	return 0;
+}
+
+static int LdSt_H_Imm_Signed(uint32_t insn, struct Insn *out)
+{
+	long long o = sd_field(insn, 23, 1);
+	long long owb;
+	if (o == 0) {
+		out->sign_extend = false;
+	} else {
+		out->sign_extend = true;
+	}
+
+	out->len = 2;
+	out->rt = sd_field(insn, 0, 5);
+	out->rn = sd_field(insn, 5, 5);
+	out->rm = 0;
+	out->of = sd_field(insn, 12, 9);
+	out->ld = sd_field(insn, 22, 1);
+	out->p = 0;
+
+	owb = sd_field(insn, 10, 2);
+	if (owb == 0)
+		out->wback = false;
+	else
+		out->wback = true;
+	return 0;
+}
+
+static int LdSt_H_Imm_UnSigned(uint32_t insn, struct Insn *out)
+{
+	long long o = sd_field(insn, 23, 1);
+	if (o == 0) {
+		out->sign_extend = false;
+	} else {
+		out->sign_extend = true;
+	}
+
+	out->len = 2;
+	out->rt = sd_field(insn, 0, 5);
+	out->rn = sd_field(insn, 5, 5);
+	out->rm = 0;
+	out->of = sd_field(insn, 10, 12);
+	out->ld = sd_field(insn, 22, 1);
+	out->p = 0;
+	out->wback = false;
+	return 0;
+}
+
+static int LdSt_H_Reg(uint32_t insn, struct Insn *out)
+{
+	long long o = sd_field(insn, 23, 1);
+	if (o == 0) {
+		out->sign_extend = false;
+	} else {
+		out->sign_extend = true;
+	}
+
+	out->len = 2;
+	out->rt = sd_field(insn, 0, 5);
+	out->rn = sd_field(insn, 5, 5);
+	out->rm = sd_field(insn, 16, 5);
+	out->of = 0;
+	out->ld = sd_field(insn, 22, 1);
+	out->p = 0;
+	out->wback = false;
+	return 0;
+}
+
+static int LdSt_Imm_Signed(uint32_t insn, struct Insn *out)
+{
+	long long size = sd_field(insn, 30, 2);
+	long long o;
+	long long owb;
+	if (size == 3) {
+		out->len = 8;
+	} else if (size == 2) {
+		out->len = 4;
+	} else {
+		pr_err("Error decoding %x\n", insn);
+		return -EFAULT;
+	}
+
+	o = sd_field(insn, 23, 1);
+	if (o == 0) {
+		out->sign_extend = false;
+	} else {
+		out->sign_extend = true;
+	}
+
+	out->rt = sd_field(insn, 0, 5);
+	out->rn = sd_field(insn, 5, 5);
+	out->rm = 0;
+	out->of = sd_field(insn, 12, 9);
+	out->ld = sd_field(insn, 22, 1);
+	out->p = sd_field(insn, 10, 1);
+
+	owb = sd_field(insn, 10, 2);
+	if (owb == 0)
+		out->wback = false;
+	else
+		out->wback = true;
+
+	return 0;
+}
+
+// b988d042
+// 1011 1001 1000
+// 1011 1000 100 //
+static int LdSt_Imm_UnSigned(uint32_t insn, struct Insn *out)
+{
+	long long size = sd_field(insn, 30, 2);
+	long long o;
+	if (size == 3) {
+		out->len = 8;
+	} else if (size == 2) {
+		out->len = 4;
+	} else {
+		pr_err("Error decoding %x\n", insn);
+		return -EFAULT;
+	}
+
+	o = sd_field(insn, 23, 1);
+	if (o == 0) {
+		out->sign_extend = false;
+		out->ld = sd_field(insn, 22, 1);
+	} else {
+		out->sign_extend = true;
+		out->ld = 1;
+	}
+
+	out->rt = sd_field(insn, 0, 5);
+	out->rn = sd_field(insn, 5, 5);
+	out->rm = 0;
+	out->of = sd_field(insn, 10, 11);
+	out->p = 0;
+	out->wback = false;
+	return 0;
+}
+
+static int LdSt_Reg(uint32_t insn, struct Insn *out)
+{
+	long long size = sd_field(insn, 30, 2);
+	long long o;
+	if (size == 3) {
+		out->len = 8;
+	} else if (size == 2) {
+		out->len = 4;
+	} else {
+		pr_err("Error decoding %x\n", insn);
+		return -EFAULT;
+	}
+
+	o = sd_field(insn, 23, 1);
+	if (o == 0) {
+		out->sign_extend = false;
+	} else {
+		out->sign_extend = true;
+	}
+
+	out->rt = sd_field(insn, 0, 5);
+	out->rn = sd_field(insn, 5, 5);
+	out->rm = sd_field(insn, 16, 5);
+	out->of = sd_field(insn, 12, 9);
+	out->ld = sd_field(insn, 22, 1);
+	out->wback = false;
+	return 0;
+}
+
+static int LdSt_Lit(uint32_t insn, struct Insn *out)
+{
+	long long size = sd_field(insn, 30, 2);
+	if (size == 0) {
+		out->len = 4;
+		out->sign_extend = false;
+	} else if (size == 1) {
+		out->len = 8;
+		out->sign_extend = false;
+	} else if (size == 2) {
+		out->len = 4;
+		out->sign_extend = true;
+	} else {
+		pr_err("Error decoding %x\n", insn);
+		return -EFAULT;
+	}
+
+	out->rt = sd_field(insn, 0, 5);
+	out->rn = -1;
+	out->rm = 0;
+	out->of = sd_field(insn, 5, 19);
+	out->ld = 0;
+	out->wback = false;
+	return 0;
+}
+
+static int LdSt_X_Reg(uint32_t insn, struct Insn *out)
+{
+	long long size = sd_field(insn, 30, 2);
+	out->sign_extend = false;
+	if (size == 2) {
+		out->len = 4;
+	} else if (size == 3) {
+		out->len = 8;
+	} else {
+		pr_err("Error decoding %x\n", insn);
+		return -EFAULT;
+	}
+
+	out->rt = sd_field(insn, 0, 5);
+	out->rt2 = -1;
+	out->rn = sd_field(insn, 5, 5);
+	out->rm = 0;
+	out->of = 0;
+	out->ld = sd_field(insn, 22, 1);
+	out->wback = false;
+	return 0;
+}
+
+static int LdSt_SW_Reg(uint32_t insn, struct Insn *out)
+{
+	long long size = sd_field(insn, 30, 2);
+	long long o;
+	long long wb;
+	out->sign_extend = false;
+	if (size == 2) {
+		out->len = 4;
+	} else if (size == 3) {
+		out->len = 8;
+	} else {
+		pr_err("Error decoding %x\n", insn);
+		return -EFAULT;
+	}
+
+	o = sd_field(insn, 23, 1);
+	if (o == 0) {
+		out->sign_extend = false;
+	} else {
+		out->sign_extend = true;
+	}
+
+	wb = sd_field(insn, 24, 2);
+	if (wb == 0)
+		out->wback = true;
+	else
+		out->wback = false;
+
+	out->rt = sd_field(insn, 0, 5);
+	out->rt2 = -1;
+	out->rn = sd_field(insn, 5, 5);
+	out->rm = 0;
+	out->of = 0;
+	out->ld = sd_field(insn, 22, 1);
+	return 0;
+}
+
+// 0 | 111 11 | 00 001 | 0 0010
+static int LdSt_PX(uint32_t insn, struct Insn *out)
+{
+	long long size = sd_field(insn, 30, 2);
+	out->sign_extend = false;
+	out->wback = false;
+	if (size == 2) {
+		out->len = 4;
+	} else if (size == 3) {
+		out->len = 8;
+	} else {
+		pr_err("Error decoding %x\n", insn);
+		return -EFAULT;
+	}
+
+	out->rt = sd_field(insn, 0, 5);
+	out->rs = sd_field(insn, 16, 5);
+	out->rn = sd_field(insn, 5, 5);
+	out->rm = 0;
+	out->of = 0;
+	out->ld = sd_field(insn, 22, 1);
+	return 0;
+}
+
+static int cache_op(uint32_t insn, struct Insn *out)
+{
+	out->cache_op = true;
+	return 0;
+}
+
+unsigned int n_decs = 12;
+struct opc decs[] = { {
+			      // 1 x 1 1 | 1 0 0 0 0 1 0
+			      // 1 x 1 1 | 1 0 0 1 0 1
+			      .mask = 0xbf200000,
+			      .val = 0xb8000000,
+			      .fp = LdSt_Imm_Signed,
+		      },
+		      {
+			      .mask = 0xbf000000,
+			      .val = 0xb9000000,
+			      .fp = LdSt_Imm_UnSigned,
+		      },
+		      {
+			      .mask = 0xbf200000,
+			      .val = 0xb8200000,
+			      .fp = LdSt_Reg,
+		      },
+		      {
+			      .mask = 0xbf000000,
+			      .val = 0x18000000,
+			      .fp = LdSt_Lit,
+		      },
+		      {
+			      .mask = 0xff200000,
+			      .val = 0x38200000,
+			      .fp = LdSt_B_Reg,
+		      },
+		      {
+			      .mask = 0xff200000,
+			      .val = 0x38000000,
+			      .fp = LdSt_B_Imm_Signed,
+		      },
+		      {
+			      .mask = 0xff200000,
+			      .val = 0x39000000,
+			      .fp = LdSt_B_Imm_UnSigned,
+		      },
+		      {
+			      .mask = 0xff200000,
+			      .val = 0x78200000,
+			      .fp = LdSt_H_Reg,
+		      },
+		      {
+			      .mask = 0xff200000,
+			      .val = 0x78000000,
+			      .fp = LdSt_H_Imm_Signed,
+		      },
+		      {
+			      .mask = 0xff000000,
+			      .val = 0x79000000,
+			      .fp = LdSt_H_Imm_UnSigned,
+		      },
+		      {
+			      .mask = 0x7e800000,
+			      .val = 0x28000000,
+			      .fp = LdSt_P_Signed,
+		      },
+		      {
+			      .mask = 0x7e800000,
+			      .val = 0x28800000,
+			      .fp = LdSt_P_UnSigned,
+		      },
+		      {
+			      // 1 x 0 0 | 1 0 0 0 | 0 1 0
+			      .mask = 0xbfe00000,
+			      .val = 0x88400000,
+			      .fp = LdSt_X_Reg,
+		      },
+		      {
+			      // 1 0 1 1 | 1 0 0 x | 1 0
+			      // 1 0 1 1 | 1 0 0 x | 1 0 0
+			      .mask = 0xfec00000,
+			      .val = 0xb8800000,
+			      .fp = LdSt_SW_Reg,
+		      },
+		      {
+			      // 1 x 0 0 | 1 0 0 0 | 0 0 0  S
+			      // 1 x 0 0 | 1 0 0 0 | 0 1 0  L
+			      .mask = 0xbfa00000,
+			      .val = 0x88000000,
+			      .fp = LdSt_PX,
+		      },
+		      {
+			      // 1 1 0 1 | 0 1 0 1 | 0 0 0 0 | 1
+			      .mask = 0xfff80000,
+			      .val = 0xd5080000,
+			      .fp = cache_op,
+		      },
+		      {
+			      .mask = 0,
+			      .val = 0,
+			      .fp = NULL,
+		      } };
+
+struct opc data_cache_inv = {
+	.mask = 0xfff9f000,
+	.val = 0xd5087000,
+};
+
+int arm64_decode(uint32_t insn, struct Insn *instr)
+{
+	unsigned int i;
+
+	if ((insn & data_cache_inv.mask) == data_cache_inv.val) {
+		return -EINVAL;
+	}
+	instr->rt2 = -1;
+	instr->wback = false;
+	instr->cache_op = false;
+	instr->rs = -1;
+	for (i = 0; decs[i].mask != 0; ++i) {
+		if ((insn & decs[i].mask) == decs[i].val) {
+			//printf("----> %d\n", i);
+			if (decs[i].fp(insn, instr) != 0) {
+				return -EFAULT;
+			}
+			instr->bin = insn;
+			return 0;
+		}
+	}
+	return -EINVAL;
+}
diff -ruN linux-5.15.0/arch/arm64/mm/decode_helper.h linux-5.15.0-patched/arch/arm64/mm/decode_helper.h
--- linux-5.15.0/arch/arm64/mm/decode_helper.h	1969-12-31 19:00:00.000000000 -0500
+++ linux-5.15.0-patched/arch/arm64/mm/decode_helper.h	2025-09-17 11:20:52.269306477 -0400
@@ -0,0 +1,35 @@
+#ifndef SIMPLE_DECODE_HELPER_H
+#define SIMPLE_DECODE_HELPER_H
+
+#include <linux/types.h>
+
+#define MAX_INSN_SIZE 1
+
+typedef u32 khwio_opcode_t;
+
+struct khwio_arch_specific_insn {
+	khwio_opcode_t *insn;
+	/* restore address after step xol */
+	unsigned long restore;
+};
+
+struct khwio_decode_ctx {
+	/* location of the probe point */
+	khwio_opcode_t *addr;
+
+	/* Offset into the symbol */
+	unsigned int offset;
+
+	/* Saved opcode (which has been replaced with breakpoint) */
+	khwio_opcode_t opcode;
+
+	struct khwio_arch_specific_insn ainsn;
+};
+
+enum khwio_insn {
+	INSN_REJECTED,
+	INSN_GOOD_NO_SLOT,
+	INSN_GOOD,
+};
+
+#endif
diff -ruN linux-5.15.0/arch/arm64/mm/fault.c linux-5.15.0-patched/arch/arm64/mm/fault.c
--- linux-5.15.0/arch/arm64/mm/fault.c	2025-09-17 10:46:55.000000000 -0400
+++ linux-5.15.0-patched/arch/arm64/mm/fault.c	2025-09-17 11:16:09.761864974 -0400
@@ -17,6 +17,7 @@
 #include <linux/init.h>
 #include <linux/kasan.h>
 #include <linux/kprobes.h>
+#include <linux/hwiotrace.h>
 #include <linux/uaccess.h>
 #include <linux/page-flags.h>
 #include <linux/sched/signal.h>
@@ -356,9 +357,43 @@
 	return (esr & ESR_ELx_FSC_TYPE) == ESR_ELx_FSC_FAULT;
 }
 
+/*
+ * Returns 0 if hwiotrace is disabled, or if the fault is not
+ * handled by hwiotrace:
+ */
+#ifdef CONFIG_HWIOTRACE
+#define TRACE_KHWIO_FAULT 0
+static inline int
+khwio_fault(struct pt_regs *regs, unsigned long addr)
+{
+	if (unlikely(is_khwio_active())) {
+#if TRACE_KHWIO_FAULT
+		printk("[hwio] khwio_fault is_khwio_active\n");
+		dump_stack();
+#endif
+		if (khwio_handler(regs, addr) == 1) {
+#if TRACE_KHWIO_FAULT
+			printk("[hwio] khwio_fault khwio_handler(regs, addr)=1\n");
+#endif
+			return -1;
+		}
+	}
+	return 0;
+}
+#else
+static inline int
+khwio_fault(struct pt_regs *regs, unsigned long addr)
+{
+	return 0;
+}
+#endif
+
+
 static void __do_kernel_fault(unsigned long addr, unsigned long esr,
 			      struct pt_regs *regs)
 {
+    if (unlikely(khwio_fault(regs, addr)))
+        return;
 	const char *msg;
 
 	/*
diff -ruN linux-5.15.0/arch/arm64/mm/hwio-mod.c linux-5.15.0-patched/arch/arm64/mm/hwio-mod.c
--- linux-5.15.0/arch/arm64/mm/hwio-mod.c	1969-12-31 19:00:00.000000000 -0500
+++ linux-5.15.0-patched/arch/arm64/mm/hwio-mod.c	2025-09-17 11:20:52.269306477 -0400
@@ -0,0 +1,1093 @@
+/*
+ * Extended mmio-mod.c for arm64 and DMA support
+ */
+
+#define pr_fmt(fmt) "hwiotrace: " fmt
+
+#define DEBUG 1
+
+#include <linux/module.h>
+#include <linux/debugfs.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <linux/interrupt.h>
+#include <linux/io.h>
+#include <linux/kallsyms.h>
+#include <asm/pgtable.h>
+#include <asm/stacktrace.h>
+#include <linux/hwiotrace.h>
+#include <linux/hwiofuzz.h>
+#include <linux/atomic.h>
+#include <linux/percpu.h>
+#include <linux/cpu.h>
+#include <linux/kcov.h>
+
+struct trap_reason {
+	unsigned long addr;
+	unsigned long ip;
+	//enum reason_type type;
+	int active_traces;
+};
+
+struct remap_trace {
+	struct list_head list;
+	struct khwio_probe probe;
+	resource_size_t phys;
+	unsigned long id;
+};
+
+/* Accessed per-cpu. */
+static DEFINE_PER_CPU(struct trap_reason, pf_reason);
+static DEFINE_PER_CPU(struct hwiotrace_rw, cpu_trace);
+
+static DEFINE_MUTEX(hwiotrace_mutex);
+static DEFINE_SPINLOCK(trace_lock);
+static atomic_t hwiotrace_enabled;
+static LIST_HEAD(trace_list); /* struct remap_trace */
+
+#define DMA_SYNC_TRACE_PROBE 1
+
+static DEFINE_SPINLOCK(sync_trace_lock);
+static LIST_HEAD(sync_trace_list); /* struct sync_trace */
+
+struct list_head *sync_trace_list_get(unsigned long *flags)
+{
+	spin_lock_irqsave(&sync_trace_lock, *flags);
+	return &sync_trace_list;
+}
+
+void sync_trace_list_put(unsigned long flags)
+{
+	spin_unlock_irqrestore(&sync_trace_lock, flags);
+}
+
+/*
+ * Locking in this file:
+ * - mmiotrace_mutex enforces enable/disable_mmiotrace() critical sections.
+ * - mmiotrace_enabled may be modified only when holding mmiotrace_mutex
+ *   and trace_lock.
+ * - Routines depending on is_enabled() must take trace_lock.
+ * - trace_list users must hold trace_lock.
+ * - is_enabled() guarantees that mmio_trace_{rw,mapping} are allowed.
+ * - pre/post callbacks assume the effect of is_enabled() being true.
+ */
+
+/* module parameters */
+static unsigned long filter_offset;
+static bool nohwiotrace;
+static const char *dma_dev;
+
+enum dma_flags {
+	HWIOTRACE_DMA_SYNC = 0x1,
+	HWIOTRACE_DMA_MAP = 0x2,
+};
+static int dma_flag;
+
+module_param(filter_offset, ulong, 0);
+module_param(nohwiotrace, bool, 0);
+
+MODULE_PARM_DESC(filter_offset, "Start address of traced mappings.");
+MODULE_PARM_DESC(nohwiotrace, "Disable actual HWIO tracing.");
+
+static bool is_enabled(void)
+{
+	return atomic_read(&hwiotrace_enabled);
+}
+
+#define REGISTER_HWIOTRACE_PROBE 1
+#define VERBOSE_LOGGING 0
+
+// TODO: not sure if this gives consistent addresses across reboots
+#define HWIOTRACE_PC_MASK (((unsigned long)1 << 20) - 1)
+
+const char *probe_type_names[] = { "DMA_STREAM", "DMA_CONSIST", "MMIO" };
+static DEFINE_SPINLOCK(probes_lock);
+static LIST_HEAD(probes);
+static LIST_HEAD(active_mappings);
+static atomic_t probe_id;
+
+struct list_head *hwiotrace_mappings_get(unsigned long *flags)
+{
+	spin_lock_irqsave(&probes_lock, *flags);
+	return &active_mappings;
+}
+
+void hwiotrace_mappings_put(unsigned long flags)
+{
+	spin_unlock_irqrestore(&probes_lock, flags);
+}
+
+struct list_head *hwiotrace_probes_get(unsigned long *flags)
+{
+	spin_lock_irqsave(&probes_lock, *flags);
+	return &probes;
+}
+
+void hwiotrace_probes_put(unsigned long flags)
+{
+	spin_unlock_irqrestore(&probes_lock, flags);
+}
+
+unsigned long get_hwio_context(void)
+{
+	static unsigned long __do_softirq_size = 0;
+
+	unsigned long xor = 0;
+
+	struct stackframe frame;
+	frame.fp = (unsigned long)__builtin_frame_address(0);
+	frame.sp = current_stack_pointer;
+	frame.pc = (unsigned long)get_hwio_context;
+
+	while (1) {
+		int ret;
+
+		ret = unwind_frame(current, &frame);
+		if (ret < 0)
+			break;
+
+		if (__do_softirq_size == 0) {
+			unsigned long offset;
+			kallsyms_lookup_size_offset((unsigned long)__do_softirq,
+						    &__do_softirq_size,
+						    &offset);
+		}
+
+		// TODO: more blacklisting
+		if (frame.pc >= (unsigned long)__do_softirq &&
+		    frame.pc < (unsigned long)__do_softirq + __do_softirq_size)
+			break;
+
+		xor ^= frame.pc &HWIOTRACE_PC_MASK;
+	}
+	return xor;
+}
+
+// must hold
+// spin_lock_irqsave(&probes_lock, flags);
+static struct hwiotrace_probe *get_probe_by_ctx(unsigned long ctx)
+{
+	struct hwiotrace_probe *pp;
+	struct hwiotrace_probe *ret_pp = NULL;
+
+	list_for_each_entry (pp, &probes, list) {
+		if (pp->ctx == ctx) {
+			ret_pp = pp;
+			break;
+		}
+	}
+	return ret_pp;
+}
+
+// must hold
+// spin_lock_irqsave(&probes_lock, flags);
+static struct hwiotrace_mapping *
+find_probe_mapping_by_va(void *va, struct hwiotrace_probe *pp)
+{
+	struct hwiotrace_mapping *am;
+	struct hwiotrace_mapping *ret_am = NULL;
+	list_for_each_entry (am, &pp->mappings, list_pp) {
+		if (am->va == va) {
+			ret_am = am;
+			break;
+		}
+	}
+	return ret_am;
+}
+
+// must hold
+// spin_lock_irqsave(&probes_lock, flags);
+static struct hwiotrace_mapping *new_mapping(void *vaddr, size_t size,
+					     struct hwiotrace_probe *pp)
+{
+	struct hwiotrace_mapping *am;
+	am = find_probe_mapping_by_va(vaddr, pp);
+	if (am == NULL) {
+		am = kmalloc(sizeof(struct hwiotrace_mapping), GFP_ATOMIC);
+		if (am == NULL) {
+			pr_err("Error allocating hwiotrace_mapping @ %p\n",
+			       vaddr);
+			return NULL;
+		}
+		am->size = size;
+		am->va = vaddr;
+		am->map_id = pp->current_map_id++;
+		am->pp = pp;
+		am->tracing = false;
+
+		// add active mapping to probe point
+		list_add(&am->list_pp, &pp->mappings);
+
+		// add active mapping to global list
+		list_add(&am->list_glob, &active_mappings);
+	}
+
+	return am;
+}
+
+// must hold
+// spin_lock_irqsave(&probes_lock, flags);
+static int remove_active_mapping_from_probe(struct hwiotrace_probe *pp,
+					    size_t map_id)
+{
+	struct hwiotrace_mapping *am;
+	struct hwiotrace_mapping *tmp;
+	list_for_each_entry_safe (am, tmp, &pp->mappings, list_pp) {
+		if (am->map_id == map_id) {
+			list_del(&am->list_pp);
+			return 0;
+		}
+	}
+	return -EFAULT;
+}
+
+int unregister_hwiotrace_probe(const char *drv_name, void *addr)
+{
+	int ret = -EFAULT;
+	unsigned long flags;
+
+	struct hwiotrace_mapping *am;
+	struct hwiotrace_mapping *tmp;
+
+#if VERBOSE_LOGGING
+	pr_info("Unregistering consistent probes @ %p\n", addr);
+#endif
+	if (!is_hwiotrace_enabled()) {
+		return -EFAULT;
+	}
+
+	if (addr == NULL) {
+		return -EFAULT;
+	}
+
+	if (drv_name != NULL && strcmp(drv_name, "18800000.qcom,icnss") != 0) {
+		return -EFAULT;
+	}
+
+	spin_lock_irqsave(&probes_lock, flags);
+
+	// TODO: can be optimized; querying all active mappings might hold
+	// CPU for long time, when this function can be called with a
+	// constrained time budget e.g., in interrupt context.
+	list_for_each_entry_safe (am, tmp, &active_mappings, list_glob) {
+		if (addr >= am->va && addr < am->va + am->size) {
+			// remove mapping from parent
+			if (remove_active_mapping_from_probe(am->pp,
+							     am->map_id) != 0) {
+				pr_err("Error failed to remove mapping %zu from parent list\n",
+				       am->map_id);
+			} else {
+				// if this probe is activated, unregister probe for this mapping
+				if (am->pp->tracing) {
+					if (am->tracing) {
+#if REGISTER_HWIOTRACE_PROBE
+						hwiotrace_sync_single_for_device(
+							(void *)am->va);
+#endif
+						am->tracing = false;
+					}
+				}
+
+				list_del(&am->list_glob);
+				kfree(am);
+				// there should not be two mappings with the same va
+				break;
+			}
+		}
+	}
+
+	spin_unlock_irqrestore(&probes_lock, flags);
+
+	return ret;
+}
+
+int register_hwiotrace_probe(const char *drv_name, void *vaddr, size_t size,
+			     enum PROBE_TYPE type)
+{
+	return __register_hwiotrace_probe(drv_name, vaddr, size, false, type);
+}
+
+int __register_hwiotrace_probe(const char *drv_name, void *vaddr, size_t size,
+			       bool transient, enum PROBE_TYPE type)
+{
+	int ret = 0;
+	unsigned long flags;
+	unsigned long ctx;
+	struct hwiotrace_probe *pp;
+	struct hwiotrace_mapping *am;
+
+	if (drv_name == NULL ||
+	    (type == DMA_STREAM &&
+	     strcmp(drv_name, "18800000.qcom,icnss") != 0)) {
+		return -EFAULT;
+	}
+
+	if (vaddr == NULL) {
+		return -EFAULT;
+	}
+
+	if (transient && !is_hwiotrace_enabled()) {
+		// Transient buffers can wait; likely be reallocated after hwiotrace is enabled
+		return -EFAULT;
+	}
+
+	ctx = get_hwio_context();
+
+	spin_lock_irqsave(&probes_lock, flags);
+
+	// get already existing probe for context if any
+	// TODO maybe add name
+	pp = get_probe_by_ctx(ctx);
+
+#if VERBOSE_LOGGING
+	pr_info("Registering stalled probe for %s @ %lx, %p (+%lx)\n", drv_name,
+		ctx, vaddr, size);
+#endif
+
+	// create new probe if none present for ctx
+	if (pp == NULL) {
+		pp = kmalloc(sizeof(struct hwiotrace_probe), GFP_ATOMIC);
+		if (pp == NULL) {
+			pr_err("Error allocating hwiotrace_probe for driver %s\n",
+			       drv_name);
+			ret = -EFAULT;
+			goto out;
+		}
+
+		// generate new unique probe id
+		atomic_inc(&probe_id);
+
+		// init probe data
+		strncpy(pp->drv_name, drv_name, MAX_STALLED_PROBE_DRV_NAME);
+		pp->ctx = ctx;
+		pp->id = atomic_read(&probe_id);
+		pp->current_map_id = 1;
+		pp->tracing = false;
+		pp->type = type;
+		pp->last_va = NULL;
+		pp->last_length = 0;
+		INIT_LIST_HEAD(&pp->mappings);
+		// add new probe point to global list
+		list_add(&pp->list, &probes);
+
+		//pr_info("Registered new stalled probe for %s @ %lx, %lx (+%lx)\n",
+		//      drv_name, pp->ctx, pp->va, pp->size);
+	}
+
+	// unregister the existing active mapping associated with the context;
+	// sometimes deallocation of a buffer is not really hookable without
+	// uselessly calling hooking function a lot of times
+	if (!transient) {
+		// generate new active mapping and add it to probe point
+		am = new_mapping(vaddr, size, pp);
+		if (am == NULL) {
+			pr_err("Error allocating hwiotrace_mapping for driver %s\n",
+			       drv_name);
+			ret = -EFAULT;
+			goto out;
+		}
+	}
+
+	// if this probe is activated, register probe for this mapping
+	if (pp->tracing) {
+#if REGISTER_HWIOTRACE_PROBE
+		bool cacheable = false;
+		if (type == DMA_STREAM) {
+			cacheable = true;
+		}
+
+		if (transient && pp->last_va != NULL) {
+			hwiotrace_sync_single_for_device(pp->last_va);
+		}
+
+		if (hwiotrace_activate_probe((void *)vaddr, size, pp->ctx,
+					     cacheable) == 0) {
+			if (am && !am->tracing) {
+				am->tracing = true;
+			}
+		}
+#endif
+	}
+
+	pp->last_va = vaddr;
+	pp->last_length = size;
+
+out:
+	spin_unlock_irqrestore(&probes_lock, flags);
+
+	return ret;
+}
+
+int activate_hwiotrace_probe(const char *drv_name, unsigned long ctx, size_t id)
+{
+	unsigned long flags;
+	struct hwiotrace_probe *pp;
+	struct hwiotrace_mapping *am;
+
+	pr_info("activate probe name: %s, ctx: 0x%lx, id: %ld\n",
+		drv_name ? drv_name : "(null)", ctx, id);
+
+	spin_lock_irqsave(&probes_lock, flags);
+	list_for_each_entry (pp, &probes, list) {
+		if (((drv_name == NULL) ||
+		     strncmp(pp->drv_name, drv_name,
+			     MAX_STALLED_PROBE_DRV_NAME) == 0) &&
+		    ((ctx == 0) || pp->ctx == ctx) &&
+		    ((id == 0) || pp->id == id)) {
+			// set flag to activate furute mappings
+			pp->tracing = true;
+			// if we do this we might hit previously freed and reallocated mappings,
+			// which are not used for dma anymore (for streaming dma)
+			// so only do that for consistent dma mappings
+			// and maybe mmio mappings, dependent on how they are used?
+			if (pp->type == DMA_CONSIST) {
+				// start tracing on all current mappings
+				list_for_each_entry (am, &pp->mappings,
+						     list_pp) {
+					if (!am->tracing) {
+#if REGISTER_HWIOTRACE_PROBE
+						if (hwiotrace_activate_probe(
+							    (void *)am->va,
+							    am->size, pp->ctx,
+							    false) == 0)
+							am->tracing = true;
+#endif
+					}
+				}
+			}
+		}
+	}
+	spin_unlock_irqrestore(&probes_lock, flags);
+
+	return 0;
+}
+
+int deactivate_hwiotrace_probe(const char *drv_name, unsigned long ctx,
+			       size_t id)
+{
+	unsigned long flags;
+	struct hwiotrace_probe *pp;
+	struct hwiotrace_mapping *am;
+
+	//pr_info("deactivate probe name: %s, ctx: 0x%lx, id: %ld, addr: 0x%lx\n",
+	//      drv_name ? drv_name : "(null)", ctx, id, addr);
+
+	spin_lock_irqsave(&probes_lock, flags);
+	list_for_each_entry (pp, &probes, list) {
+		if (((drv_name == NULL) ||
+		     strncmp(pp->drv_name, drv_name,
+			     MAX_STALLED_PROBE_DRV_NAME) == 0) &&
+		    ((ctx == 0) || pp->ctx == ctx) &&
+		    ((id == 0) || pp->id == id)) {
+			// set flag to deactivate furute mappings
+			pp->tracing = false;
+			// if we do this we might hit previously freed and reallocated mappings,
+			// which are not used for dma anymore (for streaming dma)
+			// so only do that for consistent dma mappings
+			// and maybe mmio mappings, dependent on how they are used?
+			if (pp->type == DMA_CONSIST) {
+				// start tracing on all current mappings
+				list_for_each_entry (am, &pp->mappings,
+						     list_pp) {
+					if (!am->tracing) {
+#if REGISTER_HWIOTRACE_PROBE
+						hwiotrace_sync_single_for_device(
+							(void *)am->va);
+#endif
+						am->tracing = false;
+					}
+				}
+			}
+		}
+	}
+	spin_unlock_irqrestore(&probes_lock, flags);
+	return 0;
+}
+
+int stop_all_hwiotrace_probes(void)
+{
+	return deactivate_hwiotrace_probe(NULL, 0, 0);
+	return 0;
+}
+
+int remove_all_hwiotrace_probes(void)
+{
+	// unsigned long flags;
+	// //pr_info("Unregistering consistent probes\n");
+
+	// struct hwiotrace_probe *sp;
+	// struct hwiotrace_probe *tmp;
+
+	// spin_lock_irqsave(&probes_lock, flags);
+	// list_for_each_entry_safe(sp, tmp, &probes, list) {
+	//    if(sp->active) {
+	//       hwiotrace_sync_single_for_device((void*)sp->va);
+	//    }
+	//    list_del(&sp->list);
+	//    kfree(sp);
+	// }
+	// spin_unlock_irqrestore(&probes_lock, flags);
+
+	return 0;
+}
+
+int hwio_get_some_name(char *nameout)
+{
+	struct stackframe frame;
+	unsigned long symbolsize;
+	unsigned long offset;
+	register unsigned long current_sp asm("sp");
+	char namebuf[KSYM_SYMBOL_LEN];
+	char *modname;
+	size_t nlen;
+	size_t i;
+
+	frame.fp = (unsigned long)__builtin_frame_address(0);
+	frame.sp = current_sp;
+	frame.pc = (unsigned long)hwio_get_some_name;
+
+	for (i = 0; i < 4; ++i) {
+		if (unwind_frame(current, &frame) < 0)
+			return -EFAULT;
+
+		if (i < 2)
+			continue;
+
+		if (kallsyms_lookup(frame.pc, &symbolsize, &offset, &modname,
+				    namebuf) == NULL)
+			return -EFAULT;
+
+		strncpy(nameout, namebuf, KSYM_SYMBOL_LEN);
+		nlen = strlen(namebuf);
+		nameout += nlen;
+		*nameout = '.';
+		nameout++;
+	}
+	return 0;
+}
+
+/*
+ * For some reason the pre/post pairs have been called in an
+ * unmatched order. Report and die.
+ */
+static void die_khwio_nesting_error(struct pt_regs *regs, unsigned long addr)
+{
+	const struct trap_reason *my_reason = &get_cpu_var(pf_reason);
+	pr_emerg(
+		"unexpected fault for address: 0x%08lx, last fault for address: 0x%08lx\n",
+		addr, my_reason->addr);
+	print_symbol(KERN_EMERG "faulting pc is at %s\n",
+		     instruction_pointer(regs));
+	put_cpu_var(pf_reason);
+	BUG();
+}
+
+static void pre(struct khwio_probe *p, struct pt_regs *regs, unsigned long addr,
+		struct Insn *instr)
+{
+	struct trap_reason *my_reason = &get_cpu_var(pf_reason);
+	struct hwiotrace_rw *my_trace = &get_cpu_var(cpu_trace);
+	const unsigned long instptr = instruction_pointer(regs);
+	//const enum reason_type type = get_ins_type(instptr);
+	struct dma_sync_trace *trace = p->private;
+
+	/* it doesn't make sense to have more than one active trace per cpu */
+	if (my_reason->active_traces)
+		die_khwio_nesting_error(regs, addr);
+	else
+		my_reason->active_traces++;
+
+	//my_reason->type = type;
+	my_reason->addr = addr;
+	my_reason->ip = instptr;
+
+	my_trace->phys = addr - trace->probe.addr + trace->phys;
+	my_trace->virt = addr;
+	my_trace->map_id = trace->id;
+	my_trace->pc = instptr;
+
+	/*
+	 * XXX: the timestamp recorded will be *after* the tracing has been
+	 * done, not at the time we hit the instruction. SMP implications
+	 * on event ordering?
+	 */
+
+	my_trace->opcode = instr->ld ? HWIO_READ : HWIO_WRITE;
+
+	my_trace->width = instr->len;
+	if (0 <= instr->rt2 && instr->rt2 <= 31)
+		my_trace->width *= 2;
+
+	if (my_trace->opcode == HWIO_WRITE) {
+		my_trace->value = regs->regs[instr->rt];
+	}
+
+	put_cpu_var(cpu_trace);
+	put_cpu_var(pf_reason);
+}
+
+static void post(struct khwio_probe *p, unsigned long condition,
+		 struct pt_regs *regs, struct Insn *instr)
+{
+	struct trap_reason *my_reason = &get_cpu_var(pf_reason);
+	struct hwiotrace_rw *my_trace = &get_cpu_var(cpu_trace);
+
+	/* this should always return the active_trace count to 0 */
+	my_reason->active_traces--;
+	if (my_reason->active_traces) {
+		pr_emerg("unexpected post handler");
+		BUG();
+	}
+
+	if (my_trace->opcode == HWIO_READ) {
+		my_trace->value = regs->regs[instr->rt];
+	}
+
+	hwio_trace_rw(my_trace);
+	put_cpu_var(cpu_trace);
+	put_cpu_var(pf_reason);
+}
+
+#define TRACE_DMA_SYNC_TRACE 0
+
+static int sync_for_cpu_trace_core(void *addr, unsigned long size,
+				   unsigned long ctx, bool cacheable)
+{
+	static atomic_t next_id;
+	struct dma_sync_trace *trace;
+
+	struct hwiotrace_map map = { .phys = 0,
+				     .virt = (unsigned long)addr,
+				     .pc = ctx,
+				     .len = size,
+				     .opcode = HWIO_PROBE };
+
+	unsigned long flags = GFP_KERNEL;
+
+#if 0
+	// TODO: may want to support size larger than PAGE_SIZE
+	if (size > PAGE_SIZE) {
+		return -EFAULT;
+	}
+#endif
+
+	if (in_interrupt() || irqs_disabled() || in_atomic())
+		flags = GFP_ATOMIC;
+
+	/* These are page-unaligned. */
+	trace = kmalloc(sizeof(*trace), flags);
+
+	if (!trace) {
+		pr_err("kmalloc failed in sync_for_cpu_trace_core\n");
+		return -EFAULT;
+	}
+
+#if TRACE_DMA_SYNC_TRACE
+	pr_info("sync_for_cpu_trace_core\n");
+#endif
+
+	*trace =
+		(struct dma_sync_trace){ .probe = { .addr = (unsigned long)addr,
+						    .len = size,
+						    .ctx = ctx,
+						    .pre_handler = pre,
+						    .post_handler = post,
+						    .cacheable = cacheable,
+						    .private = trace },
+					 .id = atomic_inc_return(&next_id) };
+	map.map_id = trace->id;
+
+	spin_lock_irqsave(&sync_trace_lock, flags);
+
+	hwio_trace_mapping(&map);
+	list_add_tail(&trace->list, &sync_trace_list);
+
+#if DMA_SYNC_TRACE_PROBE
+	if (!nohwiotrace)
+		register_khwio_probe(&trace->probe);
+#endif
+
+	spin_unlock_irqrestore(&sync_trace_lock, flags);
+
+	return 0;
+}
+
+static void sync_for_device_trace_core(volatile void *addr)
+{
+	struct hwiotrace_map map = { .phys = 0,
+				     .virt = (unsigned long)addr,
+				     .pc = 0, // TODO
+				     .len = 0,
+				     .opcode = HWIO_UNPROBE };
+	struct dma_sync_trace *trace;
+	struct dma_sync_trace *tmp;
+	struct dma_sync_trace *found_trace = NULL;
+
+	unsigned long flags;
+	spin_lock_irqsave(&sync_trace_lock, flags);
+
+	list_for_each_entry_safe (trace, tmp, &sync_trace_list, list) {
+		if ((unsigned long)addr >= trace->probe.addr &&
+		    (unsigned long)addr <
+			    trace->probe.addr + trace->probe.len) {
+#if DMA_SYNC_TRACE_PROBE
+			if (!nohwiotrace)
+				unregister_khwio_probe(&trace->probe);
+#endif
+			list_del(&trace->list);
+			found_trace = trace;
+			break;
+		}
+	}
+	map.map_id = (found_trace) ? found_trace->id : -1;
+	if (found_trace) {
+		hwio_trace_mapping(&map);
+	}
+
+	spin_unlock_irqrestore(&sync_trace_lock, flags);
+	if (found_trace) {
+#if TRACE_DMA_SYNC_TRACE
+		pr_info("sync_for_device_trace_core found_trace\n");
+#endif
+
+		kfree(found_trace);
+	}
+#if TRACE_DMA_SYNC_TRACE
+	else {
+		pr_info("sync_for_device_trace_core !found_trace for addr=0x%p\n",
+			addr);
+	}
+#endif
+}
+
+#define TRACE_TASK_EVENTS 1
+
+#if TRACE_TASK_EVENTS
+static struct hwiotrace_task task;
+#endif
+
+void hwiotrace_task_start(void)
+{
+	if (!is_hwiotrace_enabled())
+		return;
+
+#if TRACE_TASK_EVENTS
+	task.opcode = HWIO_TASK_START;
+	task.task_id = 1; // FIXME
+	hwio_trace_task(&task);
+#endif
+
+	if (kfuz_hwiotrace_init()) {
+#ifdef CONFIG_KCOV
+		kcov_hwiotrace_init();
+#endif
+	}
+}
+
+void hwiotrace_task_end(void)
+{
+	if (!is_hwiotrace_enabled())
+		return;
+
+#if TRACE_TASK_EVENTS
+	task.opcode = HWIO_TASK_END;
+	hwio_trace_task(&task);
+#endif
+
+	if (kfuz_hwiotrace_exit()) {
+#ifdef CONFIG_KCOV
+		kcov_hwiotrace_exit();
+#endif
+	}
+}
+
+/* APIs used by tracefs wrapper */
+int hwiotrace_activate_probe(void *addr, unsigned long size, unsigned long ctx,
+			     bool cacheable)
+{
+	return sync_for_cpu_trace_core(addr, size, ctx, cacheable);
+}
+
+/* Legacy APIs */
+void hwiotrace_sync_single_for_cpu(void *addr, unsigned long size)
+{
+	sync_for_cpu_trace_core(addr, size, 0, true);
+}
+
+void hwiotrace_sync_single_for_device(void *addr)
+{
+	sync_for_device_trace_core(addr);
+}
+
+static void ioremap_trace_core(resource_size_t offset, unsigned long size,
+			       void __iomem *addr)
+{
+	static atomic_t next_id;
+	struct remap_trace *trace = kmalloc(sizeof(*trace), GFP_KERNEL);
+
+	/* These are page-unaligned. */
+	struct hwiotrace_map map = { .phys = offset,
+				     .virt = (unsigned long)addr,
+				     .len = size,
+				     .opcode = HWIO_PROBE };
+
+	if (!trace) {
+		pr_err("kmalloc failed in ioremap\n");
+		return;
+	}
+
+	*trace = (struct remap_trace){ .probe = { .addr = (unsigned long)addr,
+						  .len = size,
+						  .pre_handler = pre,
+						  .post_handler = post,
+						  .private = trace },
+				       .phys = offset,
+				       .id = atomic_inc_return(&next_id) };
+	map.map_id = trace->id;
+
+	spin_lock_irq(&trace_lock);
+	if (!is_enabled()) {
+		kfree(trace);
+		goto not_enabled;
+	}
+
+	hwio_trace_mapping(&map);
+	list_add_tail(&trace->list, &trace_list);
+	if (!nohwiotrace)
+		register_khwio_probe(&trace->probe);
+
+not_enabled:
+	spin_unlock_irq(&trace_lock);
+}
+
+void hwiotrace_ioremap(resource_size_t offset, unsigned long size,
+		       void __iomem *addr)
+{
+	if (!is_enabled()) /* recheck and proper locking in *_core() */
+		return;
+
+	if ((filter_offset) && (offset != filter_offset))
+		return;
+	ioremap_trace_core(offset, size, addr);
+}
+
+static void iounmap_trace_core(volatile void __iomem *addr)
+{
+	struct hwiotrace_map map = { .phys = 0,
+				     .virt = (unsigned long)addr,
+				     .len = 0,
+				     .opcode = HWIO_UNPROBE };
+	struct remap_trace *trace;
+	struct remap_trace *tmp;
+	struct remap_trace *found_trace = NULL;
+
+	spin_lock_irq(&trace_lock);
+	if (!is_enabled())
+		goto not_enabled;
+
+	list_for_each_entry_safe (trace, tmp, &trace_list, list) {
+		if ((unsigned long)addr == trace->probe.addr) {
+			if (!nohwiotrace)
+				unregister_khwio_probe(&trace->probe);
+			list_del(&trace->list);
+			found_trace = trace;
+			break;
+		}
+	}
+	map.map_id = (found_trace) ? found_trace->id : -1;
+	hwio_trace_mapping(&map);
+
+not_enabled:
+	spin_unlock_irq(&trace_lock);
+	if (found_trace) {
+		synchronize_rcu(); /* unregister_khwio_probe() requirement */
+		kfree(found_trace);
+	}
+}
+
+void hwiotrace_iounmap(volatile void __iomem *addr)
+{
+	might_sleep();
+	if (is_enabled()) /* recheck and proper locking in *_core() */
+		iounmap_trace_core(addr);
+}
+
+int hwiotrace_printk(const char *fmt, ...)
+{
+	int ret = 0;
+	va_list args;
+	unsigned long flags;
+	va_start(args, fmt);
+
+	spin_lock_irqsave(&trace_lock, flags);
+	if (is_enabled())
+		ret = hwio_trace_printk(fmt, args);
+	spin_unlock_irqrestore(&trace_lock, flags);
+
+	va_end(args);
+	return ret;
+}
+
+EXPORT_SYMBOL(hwiotrace_printk);
+
+static void clear_trace_list(void)
+{
+	struct remap_trace *trace;
+	struct remap_trace *tmp;
+
+	/*
+	 * No locking required, because the caller ensures we are in a
+	 * critical section via mutex, and is_enabled() is false,
+	 * i.e. nothing can traverse or modify this list.
+	 * Caller also ensures is_enabled() cannot change.
+	 */
+	list_for_each_entry (trace, &trace_list, list) {
+		pr_notice(
+			"purging non-iounmapped trace @0x%08lx, size 0x%lx.\n",
+			trace->probe.addr, trace->probe.len);
+		if (!nohwiotrace)
+			unregister_khwio_probe(&trace->probe);
+	}
+	synchronize_rcu(); /* unregister_khwio_probe() requirement */
+
+	list_for_each_entry_safe (trace, tmp, &trace_list, list) {
+		list_del(&trace->list);
+		kfree(trace);
+	}
+}
+
+#ifdef CONFIG_HOTPLUG_CPU
+static cpumask_var_t downed_cpus;
+
+static int enter_uniprocessor(void)
+{
+	int cpu;
+	int err;
+
+	if (downed_cpus == NULL &&
+	    !alloc_cpumask_var(&downed_cpus, GFP_KERNEL)) {
+		pr_notice("Failed to allocate mask\n");
+		goto out;
+	}
+
+	get_online_cpus();
+	cpumask_copy(downed_cpus, cpu_online_mask);
+	cpumask_clear_cpu(cpumask_first(cpu_online_mask), downed_cpus);
+	if (num_online_cpus() > 1)
+		pr_notice("Disabling non-boot CPUs...\n");
+	put_online_cpus();
+
+	for_each_cpu (cpu, downed_cpus) {
+		err = cpu_down(cpu);
+		if (!err)
+			pr_info("CPU%d is down.\n", cpu);
+		else
+			pr_err("Error taking CPU%d down: %d\n", cpu, err);
+	}
+out:
+	if (num_online_cpus() > 1) {
+		pr_warning("multiple CPUs still online, may miss events.\n");
+		return -1;
+	}
+	return 0;
+}
+
+/* __ref because leave_uniprocessor calls cpu_up which is __cpuinit,
+   but this whole function is ifdefed CONFIG_HOTPLUG_CPU */
+static void __ref leave_uniprocessor(void)
+{
+	int cpu;
+	int err;
+
+	if (downed_cpus == NULL || cpumask_weight(downed_cpus) == 0)
+		return;
+	pr_notice("Re-enabling CPUs...\n");
+	for_each_cpu (cpu, downed_cpus) {
+		err = cpu_up(cpu);
+		if (!err)
+			pr_info("enabled CPU%d.\n", cpu);
+		else
+			pr_err("cannot re-enable CPU%d: %d\n", cpu, err);
+	}
+}
+
+#else /* !CONFIG_HOTPLUG_CPU */
+static int enter_uniprocessor(void)
+{
+	if (num_online_cpus() > 1) {
+		pr_warning("multiple CPUs are online, may miss events. "
+			   "Suggest booting with maxcpus=1 kernel argument.\n");
+		return -1;
+	}
+	return 0;
+}
+
+static void leave_uniprocessor(void)
+{
+}
+#endif
+
+bool is_hwiotrace_enabled(void)
+{
+	return atomic_read(&hwiotrace_enabled);
+}
+
+void enable_hwiotrace(void)
+{
+	mutex_lock(&hwiotrace_mutex);
+	if (is_enabled())
+		goto out;
+
+	khwio_init();
+	if (enter_uniprocessor() != 0)
+		goto out;
+	spin_lock_irq(&trace_lock);
+	atomic_inc(&hwiotrace_enabled);
+	spin_unlock_irq(&trace_lock);
+out:
+	mutex_unlock(&hwiotrace_mutex);
+}
+
+void activate_hwiotrace(const char *dev, int dma_sync, int dma_map, int mmio)
+{
+	mutex_lock(&hwiotrace_mutex);
+	if (!is_enabled() || !dev)
+		goto out;
+
+	spin_lock_irq(&trace_lock);
+	if (dma_sync) {
+		dma_flag = HWIOTRACE_DMA_SYNC;
+		dma_dev = dev;
+	} else if (dma_map) {
+		dma_flag = HWIOTRACE_DMA_MAP;
+		dma_dev = dev;
+	}
+	spin_unlock_irq(&trace_lock);
+	pr_info("enabled (dev=%s, dma_sync=%d, dma_map=%d, mmio=%d).\n", dev,
+		dma_sync, dma_map, mmio);
+
+out:
+	mutex_unlock(&hwiotrace_mutex);
+}
+
+void disable_hwiotrace(void)
+{
+	mutex_lock(&hwiotrace_mutex);
+	if (!is_enabled())
+		goto out;
+
+	if (dma_dev)
+		dma_dev = NULL;
+
+	spin_lock_irq(&trace_lock);
+	atomic_dec(&hwiotrace_enabled);
+	BUG_ON(is_enabled());
+	spin_unlock_irq(&trace_lock);
+
+	clear_trace_list(); /* guarantees: no more khwio callbacks */
+	leave_uniprocessor();
+	khwio_cleanup();
+	pr_info("disabled.\n");
+out:
+	mutex_unlock(&hwiotrace_mutex);
+}
diff -ruN linux-5.15.0/arch/arm64/mm/ioremap.c linux-5.15.0-patched/arch/arm64/mm/ioremap.c
--- linux-5.15.0/arch/arm64/mm/ioremap.c	2025-09-17 10:46:55.000000000 -0400
+++ linux-5.15.0-patched/arch/arm64/mm/ioremap.c	2025-09-17 11:18:45.749544584 -0400
@@ -13,10 +13,59 @@
 #include <linux/mm.h>
 #include <linux/vmalloc.h>
 #include <linux/io.h>
+#include <linux/kallsyms.h>
+#include <linux/module.h>
+#include <linux/hwiotrace.h>
 
 #include <asm/fixmap.h>
 #include <asm/tlbflush.h>
 
+#if defined(CONFIG_HWIOTRACE) && defined(CONFIG_KHWIO_MMIO)
+static void __iomem *__ioremap_caller_khwio(phys_addr_t phys_addr, size_t size,
+				      pgprot_t prot, void *caller)
+{
+	unsigned long last_addr;
+	unsigned long offset = phys_addr & ~PAGE_MASK;
+	int err;
+	unsigned long addr;
+	struct vm_struct *area;
+
+	/*
+	 * Page align the mapping address and size, taking account of any
+	 * offset.
+	 */
+	phys_addr &= PAGE_MASK;
+	size = PAGE_ALIGN(size + offset);
+
+	/*
+	 * Don't allow wraparound, zero size or outside PHYS_MASK.
+	 */
+	last_addr = phys_addr + size - 1;
+	if (!size || last_addr < phys_addr || (last_addr & ~PHYS_MASK))
+		return NULL;
+
+	area = get_vm_area_caller(size, VM_IOREMAP, caller);
+	if (!area)
+		return NULL;
+	addr = (unsigned long)area->addr;
+
+	err = ioremap_page_range(addr, addr + size, phys_addr, prot);
+	if (err) {
+		vunmap((void *)addr);
+		return NULL;
+	}
+
+	return (void __iomem *)(offset + addr);
+}
+void __iomem *__ioremap_khwio(phys_addr_t phys_addr, size_t size, pgprot_t prot)
+{
+	return __ioremap_caller_khwio(phys_addr, size, prot,
+				__builtin_return_address(0));
+}
+EXPORT_SYMBOL(__ioremap_khwio);
+#endif
+
+
 static void __iomem *__ioremap_caller(phys_addr_t phys_addr, size_t size,
 				      pgprot_t prot, void *caller)
 {
@@ -25,6 +74,9 @@
 	int err;
 	unsigned long addr;
 	struct vm_struct *area;
+#if defined(CONFIG_HWIOTRACE) && defined(CONFIG_KHWIO_MMIO)
+	char some_name[KSYM_SYMBOL_LEN * 2];
+#endif
 
 	/*
 	 * Page align the mapping address and size, taking account of any
@@ -58,6 +110,21 @@
 		return NULL;
 	}
 
+#if defined(CONFIG_HWIOTRACE) && defined(CONFIG_KHWIO_MMIO)
+	if(hwio_get_some_name(some_name) == 0) {
+		unsigned long paddr = offset + addr;
+		if(register_hwiotrace_probe(some_name, (void *)paddr, size, MMIO) != 0) {
+#if 0
+			pr_err("Error register_hwiotrace_probe(%s, 0x%lx, 0x%lx)\n",
+				some_name, paddr, size);
+#endif
+		}
+	} else {
+		pr_err("Error __ioremap could not get name to register probe\n");
+	}
+#endif
+
+
 	return (void __iomem *)(offset + addr);
 }
 
@@ -72,6 +139,15 @@
 {
 	unsigned long addr = (unsigned long)io_addr & PAGE_MASK;
 
+#if defined(CONFIG_HWIOTRACE) && defined(CONFIG_KHWIO_MMIO)
+	// we can not match on name
+	if(unregister_hwiotrace_probe(NULL, (void *)addr) != 0) {
+#if 0
+		pr_err("Error unregister_hwiotrace_probe((null), 0x%lx)\n", addr);
+#endif
+	}
+#endif
+
 	/*
 	 * We could get an address outside vmalloc range in case
 	 * of ioremap_cache() reusing a RAM mapping.
diff -ruN linux-5.15.0/arch/arm64/mm/khwio.c linux-5.15.0-patched/arch/arm64/mm/khwio.c
--- linux-5.15.0/arch/arm64/mm/khwio.c	1969-12-31 19:00:00.000000000 -0500
+++ linux-5.15.0-patched/arch/arm64/mm/khwio.c	2025-09-17 11:20:52.269306477 -0400
@@ -0,0 +1,1342 @@
+/*
+ * Extended kmmio.c for arm64 and DMA support
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/list.h>
+#include <linux/rculist.h>
+#include <linux/spinlock.h>
+#include <linux/hash.h>
+#include <linux/module.h>
+#include <linux/moduleloader.h>
+#include <linux/kallsyms.h>
+#include <linux/kcov.h>
+#include <linux/kernel.h>
+#include <linux/uaccess.h>
+#include <linux/ptrace.h>
+#include <linux/preempt.h>
+#include <linux/percpu.h>
+#include <linux/kdebug.h>
+#include <linux/mutex.h>
+#include <linux/io.h>
+#include <linux/slab.h>
+#include <asm/cacheflush.h>
+#include <asm/debug-monitors.h>
+#include <asm/tlbflush.h>
+#include <asm/traps.h>
+#include <linux/errno.h>
+#include <linux/hwiotrace.h>
+#include <linux/hwiofuzz.h>
+#include <linux/decode.h>
+#include "decode_helper.h"
+
+#define KHWIO_PAGE_HASH_BITS 4
+#define KHWIO_PAGE_TABLE_SIZE (1 << KHWIO_PAGE_HASH_BITS)
+
+#define SINGLE_STEP_ORIG_INSTR 1
+#define EMULATE_STR 0
+#define RETURN_NULL_PTE_LOOKUP 0
+#define SPLIT_FAULT_PAGE_PMDS 1
+#define ARM_FAULT_PAGE 1
+#define ADD_FAULT_PAGE 1
+#define ADD_PROBES 1
+#define DELAYED_RELEASE                                                        \
+	0 // NOT supported. May not be necessary in uniprocessor env.
+#define UNREGISTER_DISABLED 0
+#define ATOMIC_KMALLOC 1
+#define ENABLE_HWIOTRACE_HANDLERS 1
+
+struct khwio_fault_page {
+	struct list_head list;
+	struct khwio_fault_page *release_next;
+	unsigned long addr; /* the requested address */
+	pteval_t old_presence; /* page presence prior to arming */
+	bool armed;
+	unsigned long long *mapped_ptr;
+
+	/*
+	 * Number of times this page has been registered as a part
+	 * of a probe. If zero, page is disarmed and this may be freed.
+	 * Used only by writers (RCU) and post_khwio_handler().
+	 * Protected by khwio_lock, when linked into khwio_page_table.
+	 */
+	int count;
+
+	bool scheduled_for_release;
+};
+
+#if DELAYED_RELEASE
+struct khwio_delayed_release {
+	struct rcu_head rcu;
+	struct khwio_fault_page *release_list;
+};
+#endif
+
+struct khwio_context {
+	struct khwio_fault_page *fpage[2];
+	struct khwio_probe *probe;
+	unsigned long saved_flags;
+	struct Insn instr;
+	unsigned long addr;
+	bool decoded;
+	int active;
+};
+
+static DEFINE_SPINLOCK(khwio_lock);
+
+/* Protected by khwio_lock */
+unsigned int khwio_count;
+void *khwio_insn_page;
+unsigned long khwio_saved_irqflag;
+struct khwio_decode_ctx kd;
+struct pt_regs fault_regs;
+struct pt_regs khwio_regs;
+unsigned long targeted_fn_addr;
+unsigned long targeted_fn_size;
+
+/* Read-protected by RCU, write-protected by khwio_lock. */
+static struct list_head khwio_page_table[KHWIO_PAGE_TABLE_SIZE];
+static LIST_HEAD(khwio_probes);
+
+#define TRACE_LOOKUP_ADDRESS 0
+#define PT_LEVEL_PUD 1
+#define PT_LEVEL_PMD 2
+#define PT_LEVEL_PTE 3
+/*
+ * Check whether a kernel address is valid.
+ */
+pte_t *lookup_address(unsigned long addr, unsigned int *level)
+{
+#if RETURN_NULL_PTE_LOOKUP
+	return 0;
+#else
+	pgd_t *pgd;
+	pud_t *pud;
+	pmd_t *pmd;
+	pte_t *pte;
+
+	if ((((long)addr) >> VA_BITS) != -1UL)
+		return 0;
+
+	pgd = pgd_offset_k(addr);
+	if (pgd_none(*pgd)) {
+		pr_err("lookup_address pgd_none\n");
+		return 0;
+	}
+
+	pud = pud_offset(pgd, addr);
+	if (pud_none(*pud)) {
+		pr_err("lookup_address pud_none\n");
+		return 0;
+	}
+
+	if (pud_sect(*pud)) {
+		pr_info("lookup_address pud_sect pud_val(*pud)=0x%llx\n",
+			pud_val(*pud));
+		if (level)
+			*level = PT_LEVEL_PUD;
+		return (pte_t *)pud;
+	}
+
+	if (pud_bad(*pud)) {
+		pr_err("lookup_address pud_bad pud_val(*pud)=0x%llx\n",
+		       pud_val(*pud));
+		return 0;
+	}
+
+	pmd = pmd_offset(pud, addr);
+	if (pmd_none(*pmd)) {
+		pr_err("lookup_address pmd_none\n");
+		return 0;
+	}
+
+	if (pmd_sect(*pmd)) {
+		pr_info("lookup_address pmd_sect pmd_val(*pmd)=0x%llx\n",
+			pmd_val(*pmd));
+		if (level)
+			*level = PT_LEVEL_PMD;
+		return pmd;
+	}
+
+	if (pmd_bad(*pmd)) {
+		pr_err("lookup_address pmd_bad pmd_val(*pmd)=0x%llx\n",
+		       pmd_val(*pmd));
+		return 0;
+	}
+
+	pte = pte_offset_kernel(pmd, addr);
+	if (pte_none(*pte)) {
+		pr_err("lookup_address pte_none pgd_val(*pgd)=0x%llx pmd_val(*pmd)=0x%llx pte_val(*pte)=0x%llx\n",
+		       pgd_val(*pgd), pmd_val(*pmd), pte_val(*pte));
+		return 0;
+	}
+
+#if TRACE_LOOKUP_ADDRESS
+	pr_info("lookup_address pte_val(*pte)=0x%llx\n", pte_val(*pte));
+#endif
+
+	if (level)
+		*level = PT_LEVEL_PTE;
+	return pte;
+#endif
+}
+
+#define TRACE_PAGE_LIST_MGMT 0
+static struct list_head *khwio_page_list(unsigned long addr)
+{
+	unsigned int l;
+	pte_t *pte = lookup_address(addr, &l);
+
+#if TRACE_PAGE_LIST_MGMT
+	pr_info("khwio_page_list addr=0x%llx\n", addr);
+#endif
+
+	if (!pte)
+		return NULL;
+	addr &= PAGE_MASK;
+
+	return &khwio_page_table[hash_long(addr, KHWIO_PAGE_HASH_BITS)];
+}
+
+/* Accessed per-cpu */
+static DEFINE_PER_CPU(struct khwio_context, khwio_ctx);
+
+static struct khwio_probe *get_khwio_probe(unsigned long addr)
+{
+	struct khwio_probe *p;
+	struct khwio_probe *tmp;
+
+	//pr_info("get_khwio_probe addr=0x%llx\n", addr);
+
+#if !DELAYED_RELEASE
+	list_for_each_entry_safe (p, tmp, &khwio_probes, list) {
+#else
+	list_for_each_entry_rcu (p, &khwio_probes, list) {
+#endif
+		if (addr >= p->addr && addr < (p->addr + p->len))
+			return p;
+	}
+	return NULL;
+}
+
+static struct khwio_fault_page *get_khwio_fault_page(unsigned long addr)
+{
+	struct list_head *head;
+	struct khwio_fault_page *f;
+	struct khwio_fault_page *tmp;
+	unsigned int level;
+	pte_t *pte = lookup_address(addr, &level);
+
+	if (!pte)
+		return NULL;
+
+	addr &= PAGE_MASK;
+	head = khwio_page_list(addr);
+#if !DELAYED_RELEASE
+	list_for_each_entry_safe (f, tmp, head, list) {
+#else
+	list_for_each_entry_rcu (f, head, list) {
+#endif
+		if (f->addr == addr)
+			return f;
+	}
+
+	//pr_info("get_khwio_fault_page NULL\n");
+
+	return NULL;
+}
+
+#if SPLIT_FAULT_PAGE_PMDS
+static void split_pmd(pmd_t *pmd)
+{
+	// allocate new pte page table
+	pte_t *pte_pt = (pte_t *)kmalloc(PAGE_SIZE, GFP_ATOMIC);
+	unsigned long pmd_base_addr;
+	unsigned i;
+
+	pr_info("split_pmd 0x%016lx\n", (unsigned long)*pmd);
+
+	BUG_ON(!pte_pt);
+	pmd_base_addr = (unsigned long)*pmd & PMD_MASK;
+
+	*pmd = virt_to_phys(pte_pt) | PROT_NORMAL;
+	for (i = 0; i < PTRS_PER_PTE; ++i) {
+		*pte_pt = pmd_base_addr | PROT_NORMAL;
+		pte_pt++;
+		pmd_base_addr += PAGE_SIZE;
+	}
+	flush_tlb_all();
+}
+#endif
+
+#define SKIP_SINGLE_STEP 1
+
+// Fuzzer hook impl.
+static int execute_hooks(struct khwio_probe *probe, struct Insn *instr,
+			 struct pt_regs *regs, unsigned long addr,
+			 char *mapped_ptr, char *mapped_ptr_bkp)
+{
+	unsigned int page_offset;
+	unsigned int buffer_offset;
+
+	unsigned int amult = 1;
+	unsigned int access_len;
+
+	if (0 <= instr->rt2 && instr->rt2 <= 31)
+		amult = 2;
+	access_len = instr->len * amult;
+
+	// get offset from start of page
+	page_offset = addr & ~PAGE_MASK;
+	buffer_offset = addr - probe->addr;
+
+	// TODO: only flush required chache lines
+	flush_cache_all();
+
+	mapped_ptr = &mapped_ptr[page_offset];
+
+	if (instr->ld) { // instruction is a load register from memory
+		char fuzzbuf[16]; // 16 byte == maximum reach of one assembler instruction
+		memset(fuzzbuf, 0x0, 16);
+
+		if (probe &&
+		    kfuz_hwiotrace_consume(fuzzbuf, probe->addr, buffer_offset,
+					   access_len, probe->cacheable)) {
+			pr_info("fuzzing ld at 0x%x (+0x%x): 0x%016llx 0x%016llx -> 0x%016llx 0x%016llx\n",
+				buffer_offset, access_len,
+				*(long long *)mapped_ptr,
+				*(long long *)&mapped_ptr[8],
+				*(long long *)fuzzbuf,
+				*(long long *)&fuzzbuf[8]);
+			mapped_ptr = fuzzbuf;
+			mapped_ptr_bkp = NULL;
+		} else {
+			return -1;
+		}
+
+		if (emulate_ld(instr, regs, mapped_ptr, mapped_ptr_bkp) != 0) {
+			pr_err("emulation failed [%lx] addr=%lx\n",
+			       instruction_pointer(regs), addr);
+			return -1;
+		}
+
+		probe->n_read += 1;
+	} else { // instruction is a store register to memory
+		// the driver might itself overwrite some data within
+		// that mapping at a later point when the device can
+		// no longer access the area
+		if (probe &&
+		    probe->cacheable) { // TODO: check if dma streaming mapping
+			kfuz_hwiotrace_mask(probe->addr, buffer_offset,
+					    access_len);
+		}
+#if !EMULATE_STR
+		probe->n_write += 1;
+		return -1;
+#else
+		if (emulate_st(instr, regs, mapped_ptr, mapped_ptr_bkp) != 0) {
+			pr_err("emulation failed [%llx] addr=%llx\n",
+			       instruction_pointer(regs), addr);
+			return -1;
+		}
+
+		probe->n_write += 1;
+#endif
+	}
+
+	// TODO selectively flush cache line
+	flush_cache_all();
+	return SKIP_SINGLE_STEP;
+}
+
+#define NO_DECODE_AND_EMULATE 1
+
+#if !NO_DECODE_AND_EMULATE
+static int decode_and_emulate(struct khwio_probe *probe, struct pt_regs *regs,
+			      unsigned long addr)
+{
+	// this will be the instruction pointer after emulation
+	// we only handle loads and stores, so there should be no jumps
+	// get opcode
+	uint32_t insn = *(uint32_t *)instruction_pointer(regs);
+
+	unsigned int offset;
+	// get index to (long long*) mapped_ptr[PAGE_SIZE * 2]
+	unsigned int ptr_idx;
+	// get (long long) aligned offset
+	unsigned int al_offset;
+	unsigned int al_diff;
+	unsigned long long mem_data;
+	unsigned long long *original_mem_ptr = NULL;
+	unsigned long long
+		original_mem_buf[4]; // FIXME: should it be larger than 4?
+
+	struct Insn instr;
+	struct khwio_fault_page *faultpage =
+		get_khwio_fault_page(addr & PAGE_MASK);
+
+	bool manual_fuzzing = false;
+
+	if (targeted_fn_addr != 0UL &&
+	    instruction_pointer(regs) >= targeted_fn_addr &&
+	    instruction_pointer(regs) < targeted_fn_addr + targeted_fn_size) {
+		manual_fuzzing = true;
+	}
+
+	if (faultpage->mapped_ptr == NULL) {
+		return -EFAULT;
+	}
+
+#define PRINT_DECODE_FAILURE 0
+
+	if (arm64_decode(insn, &instr) != 0) {
+#if PRINT_DECODE_FAILURE
+		pr_err("decoding failed [<0x%llx>] addr=%p insn=%x\n",
+		       instruction_pointer(regs), addr, insn);
+#endif
+		return -EFAULT;
+	}
+
+	// get offset from start of page
+	offset = addr & ~PAGE_MASK;
+	// get index to (long long*) mapped_ptr[PAGE_SIZE * 2]
+	ptr_idx = offset / sizeof(unsigned long long);
+
+	original_mem_ptr = faultpage->mapped_ptr + ptr_idx;
+	memcpy(original_mem_buf, original_mem_ptr, sizeof(original_mem_buf));
+	if (probe && kfuz_hwiotrace_consume(original_mem_ptr, probe->addr,
+					    (unsigned long)original_mem_ptr -
+						    (unsigned long)probe->addr,
+					    sizeof(original_mem_buf),
+					    probe->cacheable)) {
+		pr_info("fuzzing at 0x%lx! %llx %llx %llx %llx -> %llx %llx %llx %llx\n",
+			(unsigned long)original_mem_ptr -
+				(unsigned long)probe->addr,
+			original_mem_buf[0], original_mem_buf[1],
+			original_mem_buf[2], original_mem_buf[3],
+			original_mem_ptr[0], original_mem_ptr[1],
+			original_mem_ptr[2], original_mem_ptr[3]);
+	}
+
+	if (manual_fuzzing) {
+		pr_info("manual fuzzing!\n");
+		memset(original_mem_ptr, 0xff, sizeof(unsigned long));
+	}
+
+	// get (long long) aligned offset
+	al_offset = ptr_idx * sizeof(unsigned long long);
+	al_diff = offset - al_offset;
+
+	// TODO: only flush required cache lines
+	flush_cache_all();
+
+	// get field from memory, instr.len holds number of bytes
+	// (can be byte(8), short(16), int(32) or long long(64))
+	mem_data = sd_field(faultpage->mapped_ptr[ptr_idx],
+			    al_diff * 8, // start of field
+			    instr.len); // length of field
+
+	pr_info("decode_and_emulate [<0x%lx>] insn=0x%x addr=0x%lx mem_data=0x%llx *ptr=0x%llx idx=0x%u, ptr=0x%p\n",
+		instruction_pointer(regs), insn, addr, mem_data,
+		faultpage->mapped_ptr[ptr_idx], ptr_idx, faultpage->mapped_ptr);
+
+	if (instr.ld) { // instruction is a load register from memory
+		unsigned long reg_len =
+			32; // registers can only be written as int or long long
+		unsigned long long new_data;
+		if (instr.len > 32)
+			reg_len = 64;
+		//		regs->regs[instr.rt] = 0;
+		new_data = sd_mod_field( // update first 32 bit of the 64bit register
+			regs->regs[instr.rt], // TODO: I don't know how to access the second half
+			mem_data, 0, reg_len);
+		regs->regs[instr.rt] = new_data;
+
+		// handle ldp x1, x2, [mem]
+		// ldp and stp only work on 32 or 64 bit
+		if (instr.rt2 > 0) {
+			if (instr.len == 64) {
+				// TODO: this will fail if the probed physical memory is not contigous
+				regs->regs[instr.rt2] =
+					faultpage->mapped_ptr[ptr_idx + 1];
+			} else {
+				al_diff += 4;
+				if (al_diff > 4) {
+					ptr_idx++;
+					al_diff = 0;
+				}
+				mem_data =
+					sd_field(faultpage->mapped_ptr[ptr_idx],
+						 al_diff * 8, 32);
+				//				regs->regs[instr.rt2] = 0;
+				new_data = sd_mod_field(regs->regs[instr.rt2],
+							mem_data, 0, 32);
+				regs->regs[instr.rt2] = new_data;
+			}
+		}
+	} else { // instruction is a store register to memory
+#if EMULATE_STR
+		unsigned long long new_data =
+			sd_mod_field(faultpage->mapped_ptr[ptr_idx],
+				     regs->regs[instr.rt], al_diff * 8,
+				     instr.len);
+		faultpage->mapped_ptr[ptr_idx] = new_data;
+
+		if (instr.rt2 > 0) {
+			if (instr.len == 64) {
+				faultpage->mapped_ptr[ptr_idx + 1] =
+					regs->regs[instr.rt2];
+			} else {
+				new_data = sd_mod_field(
+					faultpage->mapped_ptr[ptr_idx],
+					regs->regs[instr.rt2], 32, 32);
+				faultpage->mapped_ptr[ptr_idx] = new_data;
+			}
+		}
+#else
+		return -1;
+#endif
+	}
+
+	if (original_mem_ptr != NULL) {
+		memcpy(original_mem_ptr, original_mem_buf,
+		       sizeof(original_mem_buf));
+	}
+
+	// TODO selectively flush cache line
+	flush_cache_all();
+	return 0;
+}
+#endif
+
+#if ARM_FAULT_PAGE
+static void clear_pte_presence(pte_t *pte, bool clear, pteval_t *old)
+{
+	pteval_t v = pte_val(*pte);
+	if (clear) {
+		v &= ~PTE_VALID;
+	} else /* presume this has been called with clear==true previously */
+		v |= PTE_VALID;
+	set_pte(pte, __pte(v));
+}
+
+static int clear_page_presence(struct khwio_fault_page *f, bool clear)
+{
+	unsigned int level;
+	pte_t *pte = lookup_address(f->addr, &level);
+
+	if (!pte) {
+		pr_err("no pte for addr 0x%08lx\n", f->addr);
+		return -1;
+	}
+
+	clear_pte_presence(pte, clear, &f->old_presence);
+
+	flush_tlb_kernel_range(f->addr, f->addr + PAGE_SIZE);
+	return 0;
+}
+#endif
+
+#define LOG_ARM_DISARM 0
+
+/*
+ * Mark the given page as not present. Access to it will trigger a fault.
+ */
+static int arm_khwio_fault_page(struct khwio_fault_page *f)
+{
+	int ret;
+
+#if SPLIT_FAULT_PAGE_PMDS
+	unsigned int level;
+	pte_t *pte = lookup_address(f->addr, &level);
+	if (level == PT_LEVEL_PMD) {
+		split_pmd(pte);
+		pte = lookup_address(f->addr, &level);
+		if (level != PT_LEVEL_PTE) {
+			pr_err("split page table did not work for addr 0x%08lx\n",
+			       f->addr);
+		}
+	} else if (level != PT_LEVEL_PTE) {
+		pr_err("split page table not implemented for addr 0x%08lx\n",
+		       f->addr);
+	}
+	BUG_ON(level != PT_LEVEL_PTE);
+#endif
+
+#if ARM_FAULT_PAGE
+	WARN_ONCE(f->armed, KERN_ERR pr_fmt("khwio page already armed.\n"));
+	if (f->armed) {
+		pr_warning("double-arm: addr 0x%08lx, ref %d, old %d\n",
+			   f->addr, f->count, !!f->old_presence);
+	}
+	ret = clear_page_presence(f, true);
+	WARN_ONCE(ret < 0, KERN_ERR pr_fmt("arming at 0x%08lx failed.\n"),
+		  f->addr);
+#else
+	ret = 0;
+#endif
+#if LOG_ARM_DISARM
+	pr_info("arm_khwio_fault_page addr=0x%llx\n", f->addr);
+#endif
+	f->armed = true;
+	return ret;
+}
+
+/** Restore the given page to saved presence state. */
+static void disarm_khwio_fault_page(struct khwio_fault_page *f)
+{
+#if ARM_FAULT_PAGE
+	int ret = clear_page_presence(f, false);
+	WARN_ONCE(ret < 0, KERN_ERR "khwio disarming at 0x%08lx failed.\n",
+		  f->addr);
+#endif
+#if LOG_ARM_DISARM
+	pr_info("disarm_khwio_fault_page addr=0x%llx\n", f->addr);
+#endif
+	f->armed = false;
+}
+
+static int post_khwio_handler(unsigned long condition, struct pt_regs *regs)
+{
+	int ret = 0;
+	struct khwio_context *ctx = &get_cpu_var(khwio_ctx);
+
+	//pr_info("post_khwio_handler ctx=0x%llx\n", ctx);
+
+	if (!ctx->active) {
+		/*
+		 * debug traps without an active context are due to either
+		 * something external causing them (f.e. using a debugger while
+		 * hwio tracing enabled), or erroneous behaviour
+		 */
+		pr_emerg("unexpected debug trap on CPU %d.\n",
+			 smp_processor_id());
+		ret = -1;
+		goto out;
+	}
+
+#if ENABLE_HWIOTRACE_HANDLERS
+	if (ctx->probe && ctx->probe->post_handler && ctx->decoded)
+		ctx->probe->post_handler(ctx->probe, condition, regs,
+					 &ctx->instr);
+#endif
+
+	/* Prevent racing against release_khwio_fault_page(). */
+	//spin_lock(&khwio_lock);
+	if (ctx->fpage[0] && ctx->fpage[0]->count)
+		arm_khwio_fault_page(ctx->fpage[0]);
+
+	if (ctx->fpage[1] && ctx->fpage[1]->count)
+		arm_khwio_fault_page(ctx->fpage[1]);
+	//spin_unlock(&khwio_lock);
+
+	/* These were acquired in khwio_handler(). */
+	ctx->active--;
+	BUG_ON(ctx->active);
+
+	if (kd.ainsn.restore != 0) {
+		//pr_info("post_khwio_handler restore=0x%llx\n", kd.ainsn.restore);
+		regs->pc = kd.ainsn.restore;
+	}
+
+#if DELAYED_RELEASE
+	rcu_read_unlock();
+#endif
+	//preempt_enable_no_resched();
+	//local_irq_enable();
+
+out:
+	put_cpu_var(khwio_ctx);
+	return ret;
+}
+
+#if !SINGLE_STEP_ORIG_INSTR
+static void arch_prepare_ss_slot(struct khwio_decode_ctx *p)
+{
+	/* prepare insn slot */
+	p->ainsn.insn[0] = cpu_to_le32(p->opcode);
+
+	flush_icache_range((uintptr_t)(p->ainsn.insn),
+			   (uintptr_t)(p->ainsn.insn) +
+				   MAX_INSN_SIZE * sizeof(khwio_opcode_t));
+
+	/*
+	 * Needs restoring of return address after stepping xol.
+	 */
+	p->ainsn.restore = (unsigned long)p->addr + sizeof(khwio_opcode_t);
+}
+
+static bool aarch64_insn_is_steppable(u32 insn)
+{
+	return true;
+}
+
+/* Return:
+ *   INSN_REJECTED     If instruction is one not allowed to khwio,
+ *   INSN_GOOD         If instruction is supported and uses instruction slot,
+ *   INSN_GOOD_NO_SLOT If instruction is supported but doesn't use its slot.
+ */
+static enum khwio_insn
+arm_probe_decode_insn(khwio_opcode_t insn, struct khwio_arch_specific_insn *asi)
+{
+	/*
+	 * Instructions reading or modifying the PC won't work from the XOL
+	 * slot.
+	 */
+	if (aarch64_insn_is_steppable(insn))
+		return INSN_GOOD;
+
+#if 0
+	if (aarch64_insn_is_bcond(insn)) {
+		asi->handler = simulate_b_cond;
+	} else if (aarch64_insn_is_cbz(insn) ||
+	    aarch64_insn_is_cbnz(insn)) {
+		asi->handler = simulate_cbz_cbnz;
+	} else if (aarch64_insn_is_tbz(insn) ||
+	    aarch64_insn_is_tbnz(insn)) {
+		asi->handler = simulate_tbz_tbnz;
+	} else if (aarch64_insn_is_adr_adrp(insn)) {
+		asi->handler = simulate_adr_adrp;
+	} else if (aarch64_insn_is_b(insn) ||
+	    aarch64_insn_is_bl(insn)) {
+		asi->handler = simulate_b_bl;
+	} else if (aarch64_insn_is_br(insn) ||
+	    aarch64_insn_is_blr(insn) ||
+	    aarch64_insn_is_ret(insn)) {
+		asi->handler = simulate_br_blr_ret;
+	} else if (aarch64_insn_is_ldr_lit(insn)) {
+		asi->handler = simulate_ldr_literal;
+	} else if (aarch64_insn_is_ldrsw_lit(insn)) {
+		asi->handler = simulate_ldrsw_literal;
+	} else {
+		/*
+		 * Instruction cannot be stepped out-of-line and we don't
+		 * (yet) simulate it.
+		 */
+		return INSN_REJECTED;
+	}
+#endif
+
+	BUG();
+
+	return INSN_GOOD_NO_SLOT;
+}
+
+int arch_prepare_hwio_probe(struct khwio_decode_ctx *p, khwio_opcode_t *slot)
+{
+	unsigned long probe_addr = (unsigned long)p->addr;
+	extern char __start_rodata[];
+	extern char __end_rodata[];
+
+	if (probe_addr & 0x3)
+		return -EINVAL;
+
+	/* copy instruction */
+	p->opcode = le32_to_cpu(*p->addr);
+
+	if (in_exception_text(probe_addr))
+		return -EINVAL;
+	if (probe_addr >= (unsigned long)__start_rodata &&
+	    probe_addr <= (unsigned long)__end_rodata)
+		return -EINVAL;
+
+	/* decode instruction */
+	switch (arm_probe_decode_insn(le32_to_cpu(*p->addr), &p->ainsn)) {
+	case INSN_REJECTED: /* insn not supported */
+		// TODO: should warn
+		return -EINVAL;
+
+	case INSN_GOOD_NO_SLOT: /* insn need simulation */
+		p->ainsn.insn = NULL;
+		break;
+
+	case INSN_GOOD: /* instruction uses slot */
+		p->ainsn.insn = slot;
+		if (!p->ainsn.insn)
+			return -ENOMEM;
+		break;
+	};
+
+	/* prepare the instruction */
+	if (p->ainsn.insn)
+		arch_prepare_ss_slot(p);
+	else
+#if 1
+		BUG();
+#else
+		arch_prepare_simulate(p);
+#endif
+
+	return 0;
+}
+#endif
+
+#if 0
+static int patch_text(khwio_opcode_t *addr, u32 opcode)
+{
+	void *addrs[1];
+	u32 insns[1];
+
+	addrs[0] = (void *)addr;
+	insns[0] = (u32)opcode;
+
+	return aarch64_insn_patch_text_nosync(addrs[0], insns[0]);
+}
+#endif
+
+int khwio_handler(struct pt_regs *regs, unsigned long addr)
+{
+	struct khwio_context *ctx;
+	struct khwio_fault_page *faultpage;
+	int ret = 0; /* default to fault not handled */
+	unsigned long page_base = addr & PAGE_MASK;
+	char symbuf[KSYM_SYMBOL_LEN];
+	unsigned long flags;
+
+#if 0
+	unsigned int level;
+	pte_t *pte = lookup_address(addr, &level);
+	if (!pte) {
+		pr_err("khwio_handler pte does not exist pc=0x%llx addr=0x%llx\n", instruction_pointer(regs), addr);
+		return -EINVAL;
+	}
+#endif
+
+	local_irq_save(flags);
+	local_irq_disable();
+	//preempt_disable();
+	//spin_lock_irqsave(&khwio_lock, flags);
+#if DELAYED_RELEASE
+	rcu_read_lock();
+#endif
+
+	faultpage = get_khwio_fault_page(page_base);
+	if (!faultpage) {
+		goto no_khwio;
+	}
+
+	sprint_symbol(symbuf, instruction_pointer(regs));
+
+	ctx = &get_cpu_var(khwio_ctx);
+	if (ctx->active) {
+		if (page_base == ctx->addr) {
+			pr_debug("secondary hit for 0x%08lx CPU %d.\n", addr,
+				 smp_processor_id());
+
+			if (!faultpage->old_presence)
+				pr_info("unexpected secondary hit for address 0x%08lx on CPU %d.\n",
+					addr, smp_processor_id());
+		} else {
+			pr_emerg(
+				"recursive probe hit on CPU %d, for address 0x%08lx. Ignoring.\n",
+				smp_processor_id(), addr);
+			pr_emerg("previous hit was at 0x%08lx.\n", ctx->addr);
+			disarm_khwio_fault_page(faultpage);
+		}
+		goto no_khwio_ctx;
+	}
+	ctx->active++;
+
+	ctx->fpage[0] = faultpage;
+	ctx->fpage[1] = NULL;
+	ctx->probe = get_khwio_probe(addr);
+	ctx->addr = page_base;
+
+	/* Now we set present bit in PTE. */
+	disarm_khwio_fault_page(faultpage);
+
+	/* For the cases where a memory access spans two pages */
+	if (page_base + PAGE_SIZE - addr < 64) {
+		faultpage = get_khwio_fault_page(page_base + PAGE_SIZE);
+		if (faultpage) {
+			disarm_khwio_fault_page(faultpage);
+			ctx->fpage[1] = faultpage;
+		}
+	}
+
+	if (ctx->probe) {
+		ctx->decoded =
+			arm64_decode(*(uint32_t *)instruction_pointer(regs),
+				     &ctx->instr) == 0 ?
+				true :
+				false;
+
+		if (ctx->decoded) {
+#if ENABLE_HWIOTRACE_HANDLERS
+			// TODO: check for probe mode
+			if (ctx->probe->pre_handler) {
+				ctx->probe->pre_handler(ctx->probe, regs, addr,
+							&ctx->instr);
+			}
+#endif
+
+			if (execute_hooks(ctx->probe, &ctx->instr, regs, addr,
+					  (char *)page_base,
+					  (char *)page_base + PAGE_SIZE) ==
+			    SKIP_SINGLE_STEP) {
+				unsigned long new_pc =
+					instruction_pointer(regs) + 4;
+
+				pr_info("khwio_handler emu %s [<0x%lx>] %s addr=0x%lx\n",
+					ctx->instr.ld ? "ld" : "st",
+					instruction_pointer(regs), symbuf,
+					addr);
+
+				post_khwio_handler(0, regs);
+				regs->pc = new_pc;
+				//preempt_enable_no_resched();
+				put_cpu_var(khwio_ctx);
+				local_irq_enable();
+				return 1;
+			}
+		}
+	}
+
+	pr_info("khwio_handler %s [<0x%lx>] %s addr=0x%lx\n",
+		ctx->instr.ld ? "ld" : "?", instruction_pointer(regs), symbuf,
+		addr);
+
+	put_cpu_var(khwio_ctx);
+
+	kd.addr = (khwio_opcode_t *)instruction_pointer(regs);
+
+#if SINGLE_STEP_ORIG_INSTR
+	kd.ainsn.insn = (void *)regs->pc;
+	kd.ainsn.restore =
+		(unsigned long)kd.ainsn.insn + sizeof(khwio_opcode_t);
+#else
+	ret = arch_prepare_hwio_probe(
+		&kd, (khwio_opcode_t *)khwio_insn_page); // Must be stateless
+	//pr_info("khwio_handler addr=%p opcode=0x%x *addr=0x%x\n", kd.addr, kd.opcode, *kd.addr);
+
+	//patch_text(kd.addr, BRK64_OPCODE_HWIO_PROBES);
+#endif
+
+	memcpy(&fault_regs, regs, sizeof(struct pt_regs));
+
+	asm volatile("brk %0" : : "I"(BRK64_ESR_HWIO_PROBES));
+
+	memcpy(regs, &fault_regs, sizeof(struct pt_regs));
+
+	//spin_unlock_irqrestore(&khwio_lock, flags);
+	//preempt_enable_no_resched();
+	local_irq_restore(flags);
+	return 1; /* fault handled */
+
+no_khwio_ctx:
+	put_cpu_var(khwio_ctx);
+no_khwio:
+#if DELAYED_RELEASE
+	rcu_read_unlock();
+#endif
+	//spin_unlock_irqrestore(&khwio_lock, flags);
+	//preempt_enable_no_resched();
+	local_irq_restore(flags);
+	return ret;
+}
+
+/* You must be holding khwio_lock. */
+#if ADD_FAULT_PAGE
+static int add_khwio_fault_page(unsigned long addr)
+{
+	struct khwio_fault_page *f;
+	unsigned long flags = GFP_KERNEL;
+
+	addr &= PAGE_MASK;
+
+	f = get_khwio_fault_page(addr);
+	if (f) {
+		if (!f->count)
+			arm_khwio_fault_page(f);
+		f->count++;
+		pr_info("add_khwio_fault_page f->addr=0x%lx f->count=%u\n",
+			f->addr, f->count);
+		return 0;
+	}
+
+	if (in_interrupt() || irqs_disabled() || in_atomic())
+		flags = GFP_ATOMIC;
+
+	f = kzalloc(sizeof(*f), flags);
+	if (!f) {
+		pr_err("add_khwio_fault_page failed to allocate khwio_fault_page\n");
+		return -1;
+	}
+
+	f->count = 1;
+	f->addr = addr;
+
+	pr_info("add_khwio_fault_page f->addr=0x%lx f->count=%u\n", f->addr,
+		f->count);
+
+	if (arm_khwio_fault_page(f)) {
+		pr_err("add_khwio_fault_page failed to arm page\n");
+		kfree(f);
+		return -1;
+	}
+
+	f->mapped_ptr = (unsigned long long *)f->addr;
+
+#if !DELAYED_RELEASE
+	list_add(&f->list, khwio_page_list(f->addr));
+#else
+	list_add_rcu(&f->list, khwio_page_list(f->addr));
+#endif
+
+	return 0;
+}
+#endif
+
+/* You must be holding khwio_lock. */
+static void release_khwio_fault_page(unsigned long addr,
+				     struct khwio_fault_page **release_list)
+{
+	struct khwio_fault_page *f;
+
+	addr = addr & PAGE_MASK;
+
+	f = get_khwio_fault_page(addr);
+	if (!f)
+		return;
+
+	pr_info("release_khwio_fault_page f->addr=0x%lx f->count=%u\n", f->addr,
+		f->count);
+
+	f->count--;
+	BUG_ON(f->count < 0);
+	if (!f->count) {
+		disarm_khwio_fault_page(f);
+		if (!f->scheduled_for_release) {
+			f->release_next = *release_list;
+			*release_list = f;
+			f->scheduled_for_release = true;
+		}
+	}
+}
+
+int register_khwio_probe(struct khwio_probe *p)
+{
+	unsigned long flags;
+	int ret = 0;
+	unsigned long size = 0;
+	const unsigned long size_lim = p->len + (p->addr & ~PAGE_MASK);
+	unsigned int level;
+	pte_t *pte;
+
+	spin_lock_irqsave(&khwio_lock, flags);
+	if (get_khwio_probe(p->addr)) {
+		pr_err("register_khwio_probe already exists for addr=0x%lx\n",
+		       p->addr);
+		BUG(); // FIXME
+		ret = -EEXIST;
+		goto out;
+	}
+
+	pte = lookup_address(p->addr, &level);
+	if (!pte) {
+		pr_info("register_khwio_probe addr=0x%lx pte=null\n", p->addr);
+		ret = -EINVAL;
+		goto out;
+	}
+
+	khwio_count++;
+	pr_info("register_khwio_probe khwio_count=%u addr=0x%lx size=%lu pte=0x%llx\n",
+		khwio_count, p->addr, p->len, pte_val(*pte));
+
+#if ADD_PROBES
+	list_add(&p->list, &khwio_probes);
+#endif
+#if ADD_FAULT_PAGE
+	while (size < size_lim) {
+		if (add_khwio_fault_page(p->addr + size))
+			pr_err("Unable to set page fault.\n");
+		size += PAGE_SIZE;
+	}
+#endif
+
+#if 0
+#ifdef CONFIG_HWIOFUZZ
+	if (kfuz_hwiotrace_init()) {
+#ifdef CONFIG_KCOV
+		kcov_hwiotrace_init();
+#endif
+	}
+#endif
+#endif
+out:
+	spin_unlock_irqrestore(&khwio_lock, flags);
+	/*
+	 * XXX: What should I do here?
+	 * Here was a call to global_flush_tlb(), but it does not exist
+	 * anymore. It seems it's not needed after all.
+	 */
+	return ret;
+}
+EXPORT_SYMBOL(register_khwio_probe);
+
+#if DELAYED_RELEASE
+static void rcu_free_khwio_fault_pages(struct rcu_head *head)
+{
+	struct khwio_delayed_release *dr =
+		container_of(head, struct khwio_delayed_release, rcu);
+	struct khwio_fault_page *f = dr->release_list;
+	while (f) {
+		struct khwio_fault_page *next = f->release_next;
+		BUG_ON(f->count);
+		kfree(f);
+		f = next;
+	}
+	kfree(dr);
+}
+
+static void remove_khwio_fault_pages(struct rcu_head *head)
+{
+	struct khwio_delayed_release *dr =
+		container_of(head, struct khwio_delayed_release, rcu);
+	struct khwio_fault_page *f = dr->release_list;
+	struct khwio_fault_page **prevp = &dr->release_list;
+	unsigned long flags;
+
+	spin_lock_irqsave(&khwio_lock, flags);
+	while (f) {
+		if (!f->count) {
+			list_del_rcu(&f->list);
+			prevp = &f->release_next;
+		} else {
+			*prevp = f->release_next;
+			f->release_next = NULL;
+			f->scheduled_for_release = false;
+		}
+		f = *prevp;
+	}
+	spin_unlock_irqrestore(&khwio_lock, flags);
+
+	/* This is the real RCU destroy call. */
+	call_rcu(&dr->rcu, rcu_free_khwio_fault_pages);
+}
+#else /* DELAYED_RELEASE */
+static void remove_khwio_fault_pages(struct khwio_fault_page *release_list)
+{
+	struct khwio_fault_page *f = release_list;
+	struct khwio_fault_page **prevp = &release_list;
+	struct khwio_fault_page *prev = NULL;
+	unsigned long flags;
+
+	spin_lock_irqsave(&khwio_lock, flags);
+	while (f) {
+		if (!f->count) {
+			list_del(&f->list);
+			prevp = &f->release_next;
+			prev = f->release_next;
+			pr_info("remove_khwio_fault_pages f->addr=0x%lx *prevp=%p\n",
+				f->addr, *prevp);
+			kfree(f);
+			f = prev;
+		} else {
+			pr_err("remove_khwio_fault_pages f->addr=0x%lx FATAL ERROR\n",
+			       f->addr);
+			*prevp = f->release_next;
+			f->release_next = NULL;
+			f->scheduled_for_release = false;
+			f = *prevp;
+		}
+	}
+	spin_unlock_irqrestore(&khwio_lock, flags);
+}
+#endif /* DELAYED_RELEASE */
+
+void unregister_khwio_probe(struct khwio_probe *p)
+{
+	unsigned long flags;
+	unsigned long size = 0;
+	const unsigned long size_lim = p->len + (p->addr & ~PAGE_MASK);
+	struct khwio_fault_page *release_list = NULL;
+#if DELAYED_RELEASE
+	struct khwio_delayed_release *drelease;
+#endif
+	unsigned int level;
+	pte_t *pte;
+
+	pte = lookup_address(p->addr, &level);
+	if (!pte) {
+		return;
+	}
+#if UNREGISTER_DISABLED
+	return;
+#endif
+	if (!p) {
+		pr_err("unregister_khwio_probe p=NULL\n");
+		return;
+	}
+
+#if 0
+#ifdef CONFIG_HWIOFUZZ
+	if (kfuz_hwiotrace_exit()) {
+#ifdef CONFIG_KCOV
+		kcov_hwiotrace_exit();
+#endif
+	}
+#endif
+#endif
+
+	spin_lock_irqsave(&khwio_lock, flags);
+	while (size < size_lim) {
+		release_khwio_fault_page(p->addr + size, &release_list);
+		size += PAGE_SIZE;
+	}
+
+#if ADD_PROBES
+#if !DELAYED_RELEASE
+	list_del(&p->list);
+#else
+	list_del_rcu(&p->list);
+#endif
+#endif
+	khwio_count--;
+	spin_unlock_irqrestore(&khwio_lock, flags);
+
+	if (!release_list) {
+		pr_err("unregister_khwio_probe release_list not found for p->addr=0x%lx\n",
+		       p->addr);
+		return;
+	}
+
+	pr_info("unregister_khwio_probe khwio_count=%u p->addr=0x%lx\n",
+		khwio_count, p->addr);
+
+#if DELAYED_RELEASE
+#if ATOMIC_KMALLOC
+	drelease = kmalloc(sizeof(*drelease), GFP_ATOMIC);
+#else
+	drelease = kmalloc(sizeof(*drelease), GFP_KERNEL);
+#endif
+	if (!drelease) {
+		pr_crit("leaking khwio_fault_page objects.\n");
+		goto out;
+	}
+	drelease->release_list = release_list;
+
+	call_rcu(&drelease->rcu, remove_khwio_fault_pages);
+#else /* DELAYED_RELEASE */
+	remove_khwio_fault_pages(release_list);
+#endif /* DELAYED_RELEASE */
+}
+EXPORT_SYMBOL(unregister_khwio_probe);
+
+#define TRACE_BREAKPOINT_HANDLER 0
+
+int khwio_breakpoint_handler(struct pt_regs *regs, unsigned int esr)
+{
+#if TRACE_BREAKPOINT_HANDLER
+	pr_info("khwio_breakpoint_handler entered\n");
+#endif
+	//patch_text(kd.addr, kd.opcode);
+	regs->pc = instruction_pointer(regs) + 4;
+
+	memcpy(&khwio_regs, regs, sizeof(struct pt_regs));
+	memcpy(regs, &fault_regs, sizeof(struct pt_regs));
+
+	if (kd.ainsn.insn) {
+		/* prepare for single stepping */
+		WARN_ON(regs->pstate & PSR_D_BIT);
+
+		/* IRQs and single stepping do not mix well. */
+		khwio_saved_irqflag = regs->pstate;
+		regs->pstate |= PSR_I_BIT;
+#if TRACE_BREAKPOINT_HANDLER
+		pr_info("khwio_breakpoint_handler ainsn.insn=0x%llx ainsn.restore=0x%llx *ainsn.insn=%x\n",
+			kd.ainsn.insn, kd.ainsn.restore, *kd.ainsn.insn);
+#endif
+		kernel_enable_single_step(regs);
+#if SINGLE_STEP_ORIG_INSTR
+		kd.ainsn.insn = (void *)regs->pc;
+		kd.ainsn.restore =
+			(unsigned long)kd.ainsn.insn + sizeof(khwio_opcode_t);
+#endif
+		regs->pc = (unsigned long)kd.ainsn.insn;
+	}
+#if 0
+	else if (kd.ainsn.handler) {
+		pr_info("khwio_handler simulation\n");
+
+		/* insn simulation */
+		kd.ainsn.handler((u32)kd.opcode, (long)kd.addr, regs);
+
+		/* single step simulated, now go for post processing */
+		post_khwio_handler(0, regs);
+	}
+#endif
+	else {
+		pr_warning("khwio_breakpoint_handler unreachable\n");
+	}
+
+#if TRACE_BREAKPOINT_HANDLER
+	pr_info("khwio_breakpoint_handler exiting\n");
+#endif
+
+	return DBG_HOOK_HANDLED;
+}
+
+int khwio_single_step_handler(struct pt_regs *regs, unsigned int esr)
+{
+	unsigned long match_addr =
+		(unsigned long)kd.ainsn.insn + sizeof(khwio_opcode_t);
+
+	//pr_info("khwio_single_step_handler match_addr=0x%lx pc=0x%lx\n", match_addr, instruction_pointer(regs));
+
+	BUG_ON(match_addr !=
+	       instruction_pointer(regs)); // FIXME - this is for debugging
+
+	if (match_addr != instruction_pointer(regs))
+		return DBG_HOOK_ERROR;
+
+	//dump_stack();
+
+	if (khwio_saved_irqflag & PSR_I_BIT)
+		regs->pstate |= PSR_I_BIT;
+	else
+		regs->pstate &= ~PSR_I_BIT;
+
+	kernel_disable_single_step();
+
+	if (post_khwio_handler(0, regs) != 0) {
+		return DBG_HOOK_ERROR;
+	}
+
+	memcpy(&fault_regs, regs, sizeof(struct pt_regs));
+	memcpy(regs, &khwio_regs, sizeof(struct pt_regs));
+
+	return DBG_HOOK_HANDLED;
+}
+
+int khwio_init(void)
+{
+	int i;
+
+	for (i = 0; i < KHWIO_PAGE_TABLE_SIZE; i++)
+		INIT_LIST_HEAD(&khwio_page_table[i]);
+
+	targeted_fn_addr = 0UL;
+
+#if !SINGLE_STEP_ORIG_INSTR
+	khwio_insn_page = module_alloc(PAGE_SIZE);
+#endif
+
+	return 0;
+}
+
+void khwio_cleanup(void)
+{
+	int i;
+
+	for (i = 0; i < KHWIO_PAGE_TABLE_SIZE; i++) {
+		WARN_ONCE(
+			!list_empty(&khwio_page_table[i]), KERN_ERR
+			"khwio_page_table not empty at cleanup, any further tracing will leak memory.\n");
+	}
+
+#if !SINGLE_STEP_ORIG_INSTR
+	module_memfree(khwio_insn_page);
+#endif
+}
diff -ruN linux-5.15.0/arch/arm64/mm/Makefile linux-5.15.0-patched/arch/arm64/mm/Makefile
--- linux-5.15.0/arch/arm64/mm/Makefile	2021-10-31 16:53:10.000000000 -0400
+++ linux-5.15.0-patched/arch/arm64/mm/Makefile	2025-09-17 11:19:30.557458312 -0400
@@ -1,4 +1,8 @@
 # SPDX-License-Identifier: GPL-2.0
+KCOV_INSTRUMENT_kwhio.o := n
+KCOV_INSTRUMENT_hwio-mod.o := n
+KCOV_INSTRUMENT_decode.o := n
+
 obj-y				:= dma-mapping.o extable.o fault.o init.o \
 				   cache.o copypage.o flush.o \
 				   ioremap.o mmap.o pgd.o mmu.o \
@@ -13,3 +17,6 @@
 
 obj-$(CONFIG_KASAN)		+= kasan_init.o
 KASAN_SANITIZE_kasan_init.o	:= n
+
+obj-$(CONFIG_HWIOTRACE)		+= hwiotrace.o decode.o
+hwiotrace-y			:= khwio.o hwio-mod.o
\ No newline at end of file
diff -ruN linux-5.15.0/include/linux/decode.h linux-5.15.0-patched/include/linux/decode.h
--- linux-5.15.0/include/linux/decode.h	1969-12-31 19:00:00.000000000 -0500
+++ linux-5.15.0-patched/include/linux/decode.h	2025-09-17 11:43:54.442400749 -0400
@@ -0,0 +1,48 @@
+#ifndef SIMPLE_DECODE_H
+#define SIMPLE_DECODE_H
+
+#ifndef __KERNEL__
+#include "linux_types.h"
+#include <linux/types.h>
+#else
+#include <asm/ptrace.h>
+#endif
+
+struct Insn {
+	unsigned int rt;
+	int rt2;
+	int rs;
+	unsigned int rn;
+	unsigned int rm;
+	int of;
+	unsigned short ld;
+	unsigned short p;
+	unsigned int len;
+	unsigned int bin;
+	bool sign_extend;
+	bool wback;
+	bool cache_op;
+	// this holds the transferred value
+	// for extended trace log output
+	// and (potentially) seed generation
+	long long transfer_val[2];
+};
+
+typedef int (*fp_decode)(uint32_t, struct Insn *);
+
+struct opc {
+	uint32_t mask;
+	uint32_t val;
+	fp_decode fp;
+};
+
+int arm64_decode(uint32_t insn, struct Insn *instr);
+long long sd_field(unsigned long long insn, unsigned int start,
+		   unsigned int len);
+long long sd_mod_field(unsigned long long orig_val, unsigned long long new_val,
+		       unsigned int start, unsigned int len);
+int emulate_ld(struct Insn *instr, struct pt_regs *regs, char *data_ptr_c,
+	       char *data_ptr_c2);
+int emulate_st(struct Insn *instr, struct pt_regs *regs, char *data_ptr_c,
+	       char *data_ptr_c2);
+#endif
diff -ruN linux-5.15.0/include/linux/hwiofuzz.h linux-5.15.0-patched/include/linux/hwiofuzz.h
--- linux-5.15.0/include/linux/hwiofuzz.h	1969-12-31 19:00:00.000000000 -0500
+++ linux-5.15.0-patched/include/linux/hwiofuzz.h	2025-09-17 11:43:54.442400749 -0400
@@ -0,0 +1,37 @@
+#ifndef _LINUX_HWIOFUZZ_H
+#define _LINUX_HWIOFUZZ_H
+
+#ifdef CONFIG_HWIOFUZZ
+
+extern bool kfuz_hwiotrace_init(void);
+extern bool kfuz_hwiotrace_exit(void);
+extern bool kfuz_hwiotrace_consume(void *dest, unsigned long base, int offset,
+				   unsigned long len, bool cacheable);
+extern bool kfuz_hwiotrace_mask(unsigned long base, int offset,
+				unsigned long len);
+
+#else
+
+static inline bool kfuz_hwiotrace_init()
+{
+	return false;
+}
+static inline bool kfuz_hwiotrace_exit()
+{
+	return false;
+}
+extern inline bool kfuz_hwiotrace_consume(void *dest, unsigned long base,
+					  int offset, unsigned long len,
+					  bool cacheable)
+{
+	return false;
+}
+extern inline bool kfuz_hwiotrace_mask(unsigned long base, int offset,
+				       unsigned long len)
+{
+	return false;
+}
+
+#endif /* CONFIG_HWIOFUZZ */
+
+#endif /* _LINUX_HWIOFUZZ_H */
diff -ruN linux-5.15.0/include/linux/hwiotrace.h linux-5.15.0-patched/include/linux/hwiotrace.h
--- linux-5.15.0/include/linux/hwiotrace.h	1969-12-31 19:00:00.000000000 -0500
+++ linux-5.15.0-patched/include/linux/hwiotrace.h	2025-09-17 11:43:54.442400749 -0400
@@ -0,0 +1,235 @@
+#ifndef _LINUX_HWIOTRACE_H
+#define _LINUX_HWIOTRACE_H
+
+#include <linux/device.h>
+#include <linux/types.h>
+#include <linux/list.h>
+#include <linux/decode.h>
+
+#define MAX_STALLED_PROBE_DRV_NAME 64
+
+struct khwio_probe;
+struct pt_regs;
+
+typedef void (*khwio_pre_handler_t)(struct khwio_probe *, struct pt_regs *,
+				    unsigned long addr, struct Insn *);
+typedef void (*khwio_post_handler_t)(struct khwio_probe *,
+				     unsigned long condition, struct pt_regs *,
+				     struct Insn *);
+
+struct khwio_probe {
+	/* khwio internal list: */
+	struct list_head list;
+	/* start location of the probe point: */
+	unsigned long addr;
+	/* probe call context */
+	unsigned long ctx;
+	/* length of the probe region: */
+	unsigned long len;
+	/* Called before addr is executed: */
+	khwio_pre_handler_t pre_handler;
+	/* Called after addr is executed: */
+	khwio_post_handler_t post_handler;
+	/* how many times has this memory been read */
+	size_t n_read;
+	/* how many times has this memory been written */
+	size_t n_write;
+	/* non-cacheable memory allows for temporal modification of buffer */
+	bool cacheable;
+	void *private;
+};
+
+extern unsigned int khwio_count;
+
+extern int register_khwio_probe(struct khwio_probe *p);
+extern void unregister_khwio_probe(struct khwio_probe *p);
+extern int khwio_init(void);
+extern void khwio_cleanup(void);
+
+#ifdef CONFIG_HWIOTRACE
+/* khwio is active by some khwio_probes? */
+static inline int is_khwio_active(void)
+{
+	return khwio_count;
+}
+
+/* Called from page fault handler. */
+int khwio_handler(struct pt_regs *regs, unsigned long addr);
+int khwio_breakpoint_handler(struct pt_regs *regs, unsigned int esr);
+int khwio_single_step_handler(struct pt_regs *regs, unsigned int esr);
+
+/* Called from ioremap.c */
+extern void hwiotrace_ioremap(resource_size_t offset, unsigned long size,
+			      void __iomem *addr);
+extern void hwiotrace_iounmap(volatile void __iomem *addr);
+
+/* Called from dma-mapping.c */
+extern void hwiotrace_sync_single_for_cpu(void *addr, unsigned long size);
+extern void hwiotrace_sync_single_for_device(void *addr);
+
+int hwiotrace_activate_probe(void *addr, unsigned long size, unsigned long ctx,
+			     bool cacheable);
+
+extern void hwiotrace_free(void *addr);
+
+/* For anyone to insert markers. Remember trailing newline. */
+extern __printf(1, 2) int hwiotrace_printk(const char *fmt, ...);
+#else /* !CONFIG_HWIOTRACE: */
+static inline int is_khwio_active(void)
+{
+	return 0;
+}
+
+static inline int khwio_handler(struct pt_regs *regs, unsigned long addr)
+{
+	return 0;
+}
+
+static inline void hwiotrace_ioremap(resource_size_t offset, unsigned long size,
+				     void __iomem *addr)
+{
+}
+
+static inline void hwiotrace_iounmap(volatile void __iomem *addr)
+{
+}
+
+static inline __printf(1, 2) int hwiotrace_printk(const char *fmt, ...)
+{
+	return 0;
+}
+#endif /* CONFIG_HWIOTRACE */
+
+enum hw_io_opcode {
+	HWIO_READ = 0x1, /* struct hwiotrace_rw */
+	HWIO_WRITE = 0x2, /* struct hwiotrace_rw */
+	HWIO_PROBE = 0x3, /* struct hwiotrace_map */
+	HWIO_UNPROBE = 0x4, /* struct hwiotrace_map */
+	HWIO_TASK_START = 0x5, /* struct hwiotrace_task */
+	HWIO_TASK_END = 0x6, /* struct hwiotrace_task */
+	HWIO_UNKNOWN_OP = 0x7, /* struct hwiotrace_rw */
+};
+
+struct hwiotrace_rw {
+	resource_size_t phys; /* PCI address of register */
+	unsigned long virt;
+	unsigned long pc; /* optional program counter */
+	int map_id;
+	unsigned char width; /* size of register access in bytes */
+	unsigned long value;
+	unsigned char opcode; /* one of HWIO_{READ,WRITE,UNKNOWN_OP} */
+};
+
+struct hwiotrace_map {
+	resource_size_t phys; /* base address in PCI space */
+	unsigned long virt; /* base virtual address */
+	unsigned long pc;
+	int map_id;
+	unsigned long len; /* mapping size */
+	unsigned char opcode; /* HWIO_PROBE or HWIO_UNPROBE */
+};
+
+struct hwiotrace_task {
+	int task_id;
+	unsigned char opcode; /* HWIO_TASK_START or HWIO_TASK_END */
+};
+
+struct dma_sync_trace {
+	struct list_head list;
+	struct khwio_probe probe;
+	resource_size_t phys;
+	unsigned long id;
+};
+
+/* in kernel/trace/trace_hwiotrace.c */
+struct list_head *sync_trace_list_get(unsigned long *flags);
+void sync_trace_list_put(unsigned long flags);
+extern bool is_hwiotrace_enabled(void);
+extern void enable_hwiotrace(void);
+extern void activate_hwiotrace(const char *, int, int, int);
+extern void disable_hwiotrace(void);
+extern void hwiotrace_task_start(void);
+extern void hwiotrace_task_end(void);
+extern void hwio_trace_rw(struct hwiotrace_rw *rw);
+extern void hwio_trace_mapping(struct hwiotrace_map *map);
+extern void hwio_trace_task(struct hwiotrace_task *task);
+extern __printf(1, 0) int hwio_trace_printk(const char *fmt, va_list args);
+extern unsigned long get_hwio_context(void);
+
+enum PROBE_TYPE { DMA_STREAM, DMA_CONSIST, MMIO };
+extern const char *probe_type_names[];
+
+struct hwiotrace_probe {
+	/* khwio internal list: */
+	struct list_head list;
+	/* the name of the driver for which this probe is registered */
+	char drv_name[MAX_STALLED_PROBE_DRV_NAME];
+	/* probe call context */
+	unsigned long ctx;
+	/* unique probe id */
+	size_t id;
+	/* is the buffer currently mapped */
+	size_t current_map_id;
+	/* active mappings for this probe point */
+	struct list_head mappings;
+	/* activate future mappings for this probe point */
+	bool tracing;
+	/* the last mapped size for this context */
+	void *last_va;
+	/* the last mapped size for this context */
+	size_t last_length;
+	/* type of the probe point */
+	enum PROBE_TYPE type;
+	// TODO: @dokyung maybe we don't need this anymore?
+	// I'd rather keep this for now to optimize for heavy-traffic DMA
+	// streaming buffers. The problem is that the deallocation is not
+	// really trackable, so I cannot reasonably get memory and CPU
+	// consumption under my control
+	/* heavy-traffic transient buffers where only a single mapping can
+	   be active at any point. So we do not track active mappings of
+	   probes of this type */
+	bool transient;
+};
+
+struct hwiotrace_mapping {
+	/* probe point list: */
+	struct list_head list_pp;
+
+	/* global list: */
+	struct list_head list_glob;
+
+	/* length of the mapping */
+	unsigned long size;
+
+	/* virtual address of the mapping */
+	void *va;
+
+	/* unique mapping id */
+	size_t map_id;
+
+	/* ptr to probe point for this mapping */
+	struct hwiotrace_probe *pp;
+
+	/* currently tracing this mapping */
+	bool tracing;
+};
+
+struct list_head *hwiotrace_mappings_get(unsigned long *flags);
+void hwiotrace_mappings_put(unsigned long flags);
+struct list_head *hwiotrace_probes_get(unsigned long *flags);
+void hwiotrace_probes_put(unsigned long flags);
+int unregister_hwiotrace_probe(const char *drv_name, void *pa);
+int register_hwiotrace_probe(const char *drv_name, void *pa, size_t size,
+			     enum PROBE_TYPE type);
+int __register_hwiotrace_probe(const char *drv_name, void *pa, size_t size,
+			       bool transient, enum PROBE_TYPE);
+int unregister_hwiotrace_probe(const char *drv_name, void *pa);
+int activate_hwiotrace_probe(const char *drv_name, unsigned long ctx,
+			     size_t id);
+int deactivate_hwiotrace_probe(const char *drv_name, unsigned long ctx,
+			       size_t id);
+int stop_all_hwiotrace_probes(void);
+int remove_all_hwiotrace_probes(void);
+int hwio_get_some_name(char *nameout);
+
+#endif /* _LINUX_HWIOTRACE_H */
diff -ruN linux-5.15.0/include/linux/kcov.h linux-5.15.0-patched/include/linux/kcov.h
--- linux-5.15.0/include/linux/kcov.h	2025-09-17 10:46:55.000000000 -0400
+++ linux-5.15.0-patched/include/linux/kcov.h	2025-09-17 11:43:06.762167653 -0400
@@ -23,12 +23,22 @@
 	KCOV_MODE_TRACE_CMP = 3,
 	/* The process owns a KCOV remote reference. */
 	KCOV_MODE_REMOTE = 4,
+    KCOV_MODE_AFL = 5;
 };
 
 #define KCOV_IN_CTXSW	(1 << 30)
 
+#ifdef CONFIG_HWIOTRACE
+#define KCOV_MAP_SIZE_POW2 16
+#define KCOV_MAP_SIZE (1 << KCOV_MAP_SIZE_POW2)
+#define KCOV_AREA_SIZE KCOV_MAP_SIZE / sizeof(unsigned long)
+
+void kcov_hwiotrace_init(void);
+void kcov_hwiotrace_exit(void);
+#else
 void kcov_task_init(struct task_struct *t);
 void kcov_task_exit(struct task_struct *t);
+#endif
 
 #define kcov_prepare_switch(t)			\
 do {						\
@@ -76,6 +86,11 @@
 
 #else
 
+#ifdef CONFIG_HWIOTRACE
+static inline void kcov_hwiotrace_init() {}
+static inline void kcov_hwiotrace_exit() {}
+#endif
+
 static inline void kcov_task_init(struct task_struct *t) {}
 static inline void kcov_task_exit(struct task_struct *t) {}
 static inline void kcov_prepare_switch(struct task_struct *t) {}
diff -ruN linux-5.15.0/include/uapi/linux/kcov.h linux-5.15.0-patched/include/uapi/linux/kcov.h
--- linux-5.15.0/include/uapi/linux/kcov.h	2021-10-31 16:53:10.000000000 -0400
+++ linux-5.15.0-patched/include/uapi/linux/kcov.h	2025-09-17 11:39:23.448727374 -0400
@@ -19,9 +19,12 @@
 #define KCOV_REMOTE_MAX_HANDLES		0x100
 
 #define KCOV_INIT_TRACE			_IOR('c', 1, unsigned long)
+#define KCOV_INIT_HWIOTRACE		_IOR('c', 2, unsigned long)
 #define KCOV_ENABLE			_IO('c', 100)
 #define KCOV_DISABLE			_IO('c', 101)
 #define KCOV_REMOTE_ENABLE		_IOW('c', 102, struct kcov_remote_arg)
+#define KCOV_ENABLE_HWIOTRACE		_IO('c', 102)
+#define KCOV_DISABLE_HWIOTRACE		_IO('c', 103)
 
 enum {
 	/*
diff -ruN linux-5.15.0/kernel/exit.c linux-5.15.0-patched/kernel/exit.c
--- linux-5.15.0/kernel/exit.c	2025-09-17 10:46:55.000000000 -0400
+++ linux-5.15.0-patched/kernel/exit.c	2025-09-17 11:59:24.400349825 -0400
@@ -809,7 +809,9 @@
 	}
 
 	profile_task_exit(tsk);
-	kcov_task_exit(tsk);
+#ifndef CONFIG_HWIOTRACE
+ 	kcov_task_exit(tsk);
+#endif
 
 	ptrace_event(PTRACE_EVENT_EXIT, code);
 
diff -ruN linux-5.15.0/kernel/fork.c linux-5.15.0-patched/kernel/fork.c
--- linux-5.15.0/kernel/fork.c	2025-09-17 10:46:55.000000000 -0400
+++ linux-5.15.0-patched/kernel/fork.c	2025-09-17 11:59:10.588340330 -0400
@@ -972,7 +972,9 @@
 
 	account_kernel_stack(tsk, 1);
 
-	kcov_task_init(tsk);
+#ifndef CONFIG_HWIOTRACE
+ 	kcov_task_init(tsk);
+#endif
 	kmap_local_fork(tsk);
 
 #ifdef CONFIG_FAULT_INJECTION
diff -ruN linux-5.15.0/kernel/kcov.c linux-5.15.0-patched/kernel/kcov.c
--- linux-5.15.0/kernel/kcov.c	2025-09-17 10:46:55.000000000 -0400
+++ linux-5.15.0-patched/kernel/kcov.c	2025-09-17 12:04:56.640506902 -0400
@@ -9,6 +9,7 @@
 #include <linux/types.h>
 #include <linux/file.h>
 #include <linux/fs.h>
+#include <linux/hash.h>
 #include <linux/hashtable.h>
 #include <linux/init.h>
 #include <linux/mm.h>
@@ -56,10 +57,25 @@
 	enum kcov_mode		mode;
 	/* Size of arena (in long's). */
 	unsigned int		size;
+
+	union {
+		/* For KCOV_MODE_AFL */
+		struct {
+			/* (number of bytes) - 1, where number of
+			 * bytes must be a power of 2. */
+			unsigned int mask;
+
+		/* Previous PC (for KCOV_MODE_AFL). */
+			unsigned int prev_location;
+		};
+	};
+
 	/* Coverage buffer shared with user space. */
 	void			*area;
-	/* Task for which we collect coverage, or NULL. */
-	struct task_struct	*t;
+#ifndef CONFIG_HWIOTRACE
+ 	/* Task for which we collect coverage, or NULL. */
+ 	struct task_struct	*t;
+#endif
 	/* Collecting coverage from remote (background) threads. */
 	bool			remote;
 	/* Size of remote area (in long's). */
@@ -191,6 +207,142 @@
 	return ip;
 }
 
+#ifdef CONFIG_HWIOTRACE
+static struct kcov *kcov_hwiotrace;
+static void *kcov_area;
+static bool kcov_active;
+
+static inline unsigned long canonicalize_ip(unsigned long ip)
+{
+#ifdef CONFIG_RANDOMIZE_BASE
+	//ip -= kaslr_offset(); // FIXME: should be per-module
+#endif
+	return ip;
+}
+
+#define FF(_b) (0xff << ((_b) << 3))
+
+static u32 count_bytes(u8 *mem) {
+	u32 *ptr = (u32 *)mem;
+	u32 i = (KCOV_MAP_SIZE >> 2);
+	u32 ret = 0;
+
+	while (i--) {
+		u32 v = *(ptr++);
+
+		if (!v) continue;
+		if (v & FF(0)) ret++;
+		if (v & FF(1)) ret++;
+		if (v & FF(2)) ret++;
+		if (v & FF(3)) ret++;
+	}
+	return ret;
+}
+
+#define KCOV_HWIOTRACE_SHIFT 56
+#define KCOV_HWIOTRACE_MASK ~((1UL << KCOV_HWIOTRACE_SHIFT) - 1)
+#define KCOV_HWIOTRACE_UNKNOWN 0x0UL << KCOV_HWIOTRACE_SHIFT
+#define KCOV_HWIOTRACE_HARDIRQ 0x1UL << KCOV_HWIOTRACE_SHIFT
+#define KCOV_HWIOTRACE_SOFTIRQ 0x2UL << KCOV_HWIOTRACE_SHIFT
+#define KCOV_HWIOTRACE_KWORKER 0x3UL << KCOV_HWIOTRACE_SHIFT
+#define KCOV_HWIOTRACE_KTHREAD 0x4UL << KCOV_HWIOTRACE_SHIFT
+#define KCOV_HWIOTRACE_UTHREAD 0x5UL << KCOV_HWIOTRACE_SHIFT
+
+void __sanitizer_cov_trace_pc(void)
+{
+	unsigned long location = canonicalize_ip(_RET_IP_);
+	struct kcov *kcov = kcov_hwiotrace;
+
+	if (!kcov)
+		return;
+
+	if (!kcov_area)
+		return;
+
+	/* BB coverage represented as an array of instruction pointers */
+	if (kcov->mode == KCOV_MODE_TRACE) {
+		unsigned long *area = kcov_area;
+		unsigned long pos;
+		unsigned long flag = KCOV_HWIOTRACE_UNKNOWN;
+
+		if (in_irq()) {
+			flag = KCOV_HWIOTRACE_HARDIRQ;
+		}
+		else if (in_softirq()) {
+			flag = KCOV_HWIOTRACE_SOFTIRQ;
+		}
+		else if (current) {
+			if (current->flags & PF_WQ_WORKER) {
+				flag = KCOV_HWIOTRACE_KWORKER;
+			}
+			else if (current->flags & PF_KTHREAD) {
+				flag = KCOV_HWIOTRACE_KTHREAD;
+			}
+			else {
+				// conservatively say it's a user thread
+				flag = KCOV_HWIOTRACE_UTHREAD;
+			}
+		}
+
+		/* The first word is number of subsequent PCs. */
+		pos = READ_ONCE(area[0]) + 1;
+		if (likely(pos < kcov->size)) {
+			area[pos] = _RET_IP_ & ~KCOV_HWIOTRACE_MASK;
+			area[pos] |= flag;
+			WRITE_ONCE(area[0], pos);
+		}
+	}
+	/* AFL-style edge coverage */
+	else if (kcov->mode == KCOV_MODE_AFL) {
+		unsigned char *area;
+
+		if (!kcov_active)
+			return;
+
+		area = kcov_area;
+
+		++area[(kcov->prev_location ^ location) & kcov->mask];
+		kcov->prev_location = hash_long(location, BITS_PER_LONG);
+	}
+}
+EXPORT_SYMBOL(__sanitizer_cov_trace_pc);
+
+void kcov_hwiotrace_init(void)
+{
+	struct kcov *kcov = kcov_hwiotrace;
+
+	if (!kcov) {
+		return;
+	}
+
+	kcov_active = true;
+
+	if (!kcov_area) {
+		return;
+	}
+
+	memset(kcov_area, 0, KCOV_MAP_SIZE);
+
+	pr_info("kcov_hwiotrace_init kcov=%p kcov->area=%p count=%u\n", kcov, kcov_area, count_bytes(kcov_area));
+}
+
+void kcov_hwiotrace_exit(void)
+{
+	struct kcov *kcov = kcov_hwiotrace;
+
+	if (!kcov) {
+		return;
+	}
+
+	kcov_active = false;
+
+	if (!kcov_area) {
+		return;
+	}
+
+	pr_info("kcov_hwiotrace_exit kcov=%p kcov_area=%p count=%u\n", kcov, kcov_area, count_bytes(kcov_area));
+}
+#else
 /*
  * Entry point from instrumented code.
  * This is called once per basic-block/edge.
@@ -362,11 +514,13 @@
 	t->kcov_handle = 0;
 }
 
+#ifndef CONFIG_HWIOTRACE
 void kcov_task_init(struct task_struct *t)
 {
 	kcov_task_reset(t);
 	t->kcov_handle = current->kcov_handle;
 }
+#endif
 
 static void kcov_reset(struct kcov *kcov)
 {
@@ -569,7 +723,9 @@
 static int kcov_ioctl_locked(struct kcov *kcov, unsigned int cmd,
 			     unsigned long arg)
 {
-	struct task_struct *t;
+#ifndef CONFIG_HWIOTRACE
+ 	struct task_struct *t;
+#endif
 	unsigned long size, unused;
 	int mode, i;
 	struct kcov_remote_arg *remote_arg;
@@ -577,7 +733,40 @@
 	unsigned long flags;
 
 	switch (cmd) {
+#ifdef CONFIG_HWIOTRACE
+	case KCOV_INIT_HWIOTRACE:
+		if (kcov->mode != KCOV_MODE_DISABLED)
+			return -EBUSY;
+		size = arg;
+		if (size < 2 || size > INT_MAX / sizeof(unsigned long))
+			return -EINVAL;
+		kcov->size = size;
+		kcov->mask = (size * sizeof(unsigned long)) - 1;
+		kcov->prev_location = hash_long(0, BITS_PER_LONG);
+		kcov->mode = KCOV_MODE_AFL;
+		return 0;
+	case KCOV_ENABLE_HWIOTRACE:
+		unused = arg;
+		if (unused != 0 || kcov->mode == KCOV_MODE_DISABLED ||
+			kcov->area == NULL)
+			return -EINVAL;
+		kcov_hwiotrace = kcov;
+		kcov_area = kcov->area;
+		kcov_get(kcov);
+		return 0;
+	case KCOV_DISABLE_HWIOTRACE:
+		unused = arg;
+		if (unused != 0)
+			return -EINVAL;
+		kcov_hwiotrace = NULL;
+		kcov_area = NULL;
+		kcov_active = false;
+		kcov_put(kcov);
+		return 0;
+#endif
 	case KCOV_INIT_TRACE:
+		pr_info("kcov_ioctl: KCOV_INIT_TRACE arg=%lu\n", arg);
+
 		/*
 		 * Enable kcov in trace mode and setup buffer size.
 		 * Must happen before anything else.
@@ -595,6 +784,7 @@
 		kcov->size = size;
 		kcov->mode = KCOV_MODE_INIT;
 		return 0;
+#ifndef CONFIG_HWIOTRACE
 	case KCOV_ENABLE:
 		/*
 		 * Enable coverage for the current task.
@@ -630,6 +820,7 @@
 		kcov_disable(t, kcov);
 		kcov_put(kcov);
 		return 0;
+#endif
 	case KCOV_REMOTE_ENABLE:
 		if (kcov->mode != KCOV_MODE_INIT || !kcov->area)
 			return -EINVAL;
diff -ruN linux-5.15.0/kernel/softirq.c linux-5.15.0-patched/kernel/softirq.c
--- linux-5.15.0/kernel/softirq.c	2025-09-17 10:46:55.000000000 -0400
+++ linux-5.15.0-patched/kernel/softirq.c	2025-09-17 12:06:41.996535288 -0400
@@ -26,6 +26,7 @@
 #include <linux/smpboot.h>
 #include <linux/tick.h>
 #include <linux/irq.h>
+#include <linux/hwiotrace.h>
 #include <linux/wait_bit.h>
 
 #include <asm/softirq_stack.h>
@@ -573,7 +574,17 @@
 		kstat_incr_softirqs_this_cpu(vec_nr);
 
 		trace_softirq_entry(vec_nr);
+
+#ifdef CONFIG_HWIOTRACE
+		hwiotrace_task_start();
+#endif
+
 		h->action(h);
+
+#ifdef CONFIG_HWIOTRACE
+		hwiotrace_task_end();
+#endif
+
 		trace_softirq_exit(vec_nr);
 		if (unlikely(prev_count != preempt_count())) {
 			pr_err("huh, entered softirq %u %s %p with preempt_count %08x, exited with %08x?\n",
diff -ruN linux-5.15.0/kernel/trace/Kconfig linux-5.15.0-patched/kernel/trace/Kconfig
--- linux-5.15.0/kernel/trace/Kconfig	2025-09-17 10:46:55.000000000 -0400
+++ linux-5.15.0-patched/kernel/trace/Kconfig	2025-09-17 11:51:37.639805594 -0400
@@ -970,6 +970,26 @@
 
 	  Say N, unless you absolutely know what you are doing.
 
+config HWIOTRACE
+	bool "HW IO tracing"
+	depends on PCI
+	select GENERIC_TRACER
+	help
+	  Hwiotrace traces HW I/O access and is meant for testing.
+	  Tracing is disabled by default and can be enabled at run-time.
+
+	  If you are not helping to develop drivers, say N.
+
+config HWIOFUZZ
+	bool "HW IO fuzzing"
+	depends on PCI
+	select GENERIC_TRACER
+	help
+	  Hwiofuzz fuzzes HW I/O access and is meant for testing.
+	  Fuzzing is disabled by default and can be enabled at run-time.
+
+	  If you are not helping to develop drivers, say N.
+
 config PREEMPTIRQ_DELAY_TEST
 	tristate "Test module to create a preempt / IRQ disable delay thread to test latency tracers"
 	depends on m
diff -ruN linux-5.15.0/kernel/trace/Makefile linux-5.15.0-patched/kernel/trace/Makefile
--- linux-5.15.0/kernel/trace/Makefile	2025-09-17 10:46:55.000000000 -0400
+++ linux-5.15.0-patched/kernel/trace/Makefile	2025-09-17 11:52:02.875850539 -0400
@@ -63,6 +63,8 @@
 obj-$(CONFIG_NOP_TRACER) += trace_nop.o
 obj-$(CONFIG_STACK_TRACER) += trace_stack.o
 obj-$(CONFIG_MMIOTRACE) += trace_mmiotrace.o
+obj-$(CONFIG_HWIOTRACE) += trace_hwiotrace.o
+obj-$(CONFIG_HWIOFUZZ) += trace_hwiofuzz.o
 obj-$(CONFIG_FUNCTION_GRAPH_TRACER) += trace_functions_graph.o
 obj-$(CONFIG_TRACE_BRANCH_PROFILING) += trace_branch.o
 obj-$(CONFIG_BLK_DEV_IO_TRACE) += blktrace.o
diff -ruN linux-5.15.0/kernel/trace/trace_entries.h linux-5.15.0-patched/kernel/trace/trace_entries.h
--- linux-5.15.0/kernel/trace/trace_entries.h	2021-10-31 16:53:10.000000000 -0400
+++ linux-5.15.0-patched/kernel/trace/trace_entries.h	2025-09-17 11:53:03.579949524 -0400
@@ -289,6 +289,63 @@
 		 __entry->map_id, __entry->opcode)
 );
 
+FTRACE_ENTRY(hwiotrace_rw, trace_hwiotrace_rw,
+
+	TRACE_HWIO_RW,
+
+	F_STRUCT(
+		__field_struct(	struct hwiotrace_rw,	rw	)
+		__field_desc(	resource_size_t, rw,	phys	)
+		__field_desc(	unsigned long,	rw,	value	)
+		__field_desc(	unsigned long,	rw,	pc	)
+		__field_desc(	int, 		rw,	map_id	)
+		__field_desc(	unsigned char,	rw,	opcode	)
+		__field_desc(	unsigned char,	rw,	width	)
+	),
+
+	F_printk("%lx %lx %lx %d %x %x",
+		 (unsigned long)__entry->phys, __entry->value, __entry->pc,
+		 __entry->map_id, __entry->opcode, __entry->width),
+
+	FILTER_OTHER
+);
+
+FTRACE_ENTRY(hwiotrace_map, trace_hwiotrace_map,
+
+	TRACE_HWIO_MAP,
+
+	F_STRUCT(
+		__field_struct(	struct hwiotrace_map,	map	)
+		__field_desc(	resource_size_t, map,	phys	)
+		__field_desc(	unsigned long,	map,	virt	)
+		__field_desc(	unsigned long,	map,	len	)
+		__field_desc(	int, 		map,	map_id	)
+		__field_desc(	unsigned char,	map,	opcode	)
+	),
+
+	F_printk("%lx %lx %lx %d %x",
+		 (unsigned long)__entry->phys, __entry->virt, __entry->len,
+		 __entry->map_id, __entry->opcode),
+
+	FILTER_OTHER
+);
+
+FTRACE_ENTRY(hwiotrace_task, trace_hwiotrace_task,
+
+	TRACE_HWIO_TASK,
+
+	F_STRUCT(
+		__field_struct(	struct hwiotrace_task,	task	)
+		__field_desc(	int, 		task,	task_id	)
+		__field_desc(	unsigned char,	task,	opcode	)
+	),
+
+	F_printk("%d %x",
+		 __entry->task_id, __entry->opcode),
+
+	FILTER_OTHER
+);
+
 
 #define TRACE_FUNC_SIZE 30
 #define TRACE_FILE_SIZE 20
diff -ruN linux-5.15.0/kernel/trace/trace.h linux-5.15.0-patched/kernel/trace/trace.h
--- linux-5.15.0/kernel/trace/trace.h	2025-09-17 10:46:55.000000000 -0400
+++ linux-5.15.0-patched/kernel/trace/trace.h	2025-09-17 11:56:50.624225950 -0400
@@ -9,6 +9,7 @@
 #include <linux/clocksource.h>
 #include <linux/ring_buffer.h>
 #include <linux/mmiotrace.h>
+#include <linux/hwiotrace.h>
 #include <linux/tracepoint.h>
 #include <linux/ftrace.h>
 #include <linux/trace.h>
@@ -43,6 +44,9 @@
 	TRACE_BPRINT,
 	TRACE_MMIO_RW,
 	TRACE_MMIO_MAP,
+    TRACE_HWIO_RW,
+	TRACE_HWIO_MAP,
+	TRACE_HWIO_TASK,
 	TRACE_BRANCH,
 	TRACE_GRAPH_RET,
 	TRACE_GRAPH_ENT,
@@ -472,6 +476,12 @@
 			  TRACE_MMIO_RW);				\
 		IF_ASSIGN(var, ent, struct trace_mmiotrace_map,		\
 			  TRACE_MMIO_MAP);				\
+		IF_ASSIGN(var, ent, struct trace_hwiotrace_rw,		\
+			  TRACE_HWIO_RW);				\
+		IF_ASSIGN(var, ent, struct trace_hwiotrace_map,		\
+			  TRACE_HWIO_MAP);				\
+		IF_ASSIGN(var, ent, struct trace_hwiotrace_task,	\
+			  TRACE_HWIO_TASK);		\
 		IF_ASSIGN(var, ent, struct trace_branch, TRACE_BRANCH); \
 		IF_ASSIGN(var, ent, struct ftrace_graph_ent_entry,	\
 			  TRACE_GRAPH_ENT);		\
diff -ruN linux-5.15.0/kernel/trace/trace_hwiofuzz.c linux-5.15.0-patched/kernel/trace/trace_hwiofuzz.c
--- linux-5.15.0/kernel/trace/trace_hwiofuzz.c	1969-12-31 19:00:00.000000000 -0500
+++ linux-5.15.0-patched/kernel/trace/trace_hwiofuzz.c	2025-09-17 11:55:25.424137380 -0400
@@ -0,0 +1,562 @@
+/*
+ * HW I/O fuzzing
+ */
+
+#define pr_fmt(fmt) "hwiofuzz: " fmt
+
+#include <linux/compiler.h>
+#include <linux/types.h>
+#include <linux/file.h>
+#include <linux/fs.h>
+#include <linux/hash.h>
+#include <linux/mm.h>
+#include <linux/printk.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+#include <linux/vmalloc.h>
+#include <linux/debugfs.h>
+#include <linux/uaccess.h>
+
+#define HWIOFUZZ_PER_CTX_STATIC 1
+#define HWIOFUZZ_PER_IRQ_DYNAMIC 2
+
+#define HWIOFUZZ_MAPPING HWIOFUZZ_PER_IRQ_DYNAMIC
+
+/*
+ * Must be synchronized with the executor program
+ */
+#define HWIOFUZZ_INIT _IOR('c', 1, unsigned long)
+#define HWIOFUZZ_CONSUME_ENABLE _IO('c', 100)
+#define HWIOFUZZ_CONSUME_CHECK _IO('c', 101)
+#define HWIOFUZZ_CONSUME_DISABLE _IO('c', 102)
+
+enum kfuz_mode {
+	HWIOFUZZ_MODE_DISABLED = 0,
+	HWIOFUZZ_MODE_TRACE = 1,
+};
+
+/*
+ * kfuz descriptor (one per opened debugfs file).
+ * State transitions of the descriptor:
+ *  - initial state after open()
+ *  - then there must be a single ioctl(HWIOFUZZ_INIT_TRACE) call
+ *  - then, mmap() call (several calls are allowed but not useful)
+ *  - then, repeated enable/disable for a task (only one task a time allowed)
+ */
+struct kfuz {
+	/*
+	 * Reference counter. We keep one for:
+	 *  - opened file descriptor
+	 *  - task with enabled kfuz (we can't unwire it from another task)
+	 */
+	atomic_t refcount;
+	/* The lock protects mode, size, area and t. */
+	spinlock_t lock;
+	enum kfuz_mode mode;
+	/* Size of arena (in long's for HWIOFUZZ_MODE_TRACE). */
+	unsigned size;
+	/* Mutation buffer shared with user space. */
+	void *area;
+	/* Task from which we get mutation, or NULL. */
+	struct task_struct *t;
+
+	bool enabled;
+	bool inited;
+	bool consumed;
+	long highwatermark;
+};
+
+static struct kfuz *hwiofuzz;
+static void *kfuz_area;
+
+#if HWIOFUZZ_MAPPING == HWIOFUZZ_PER_CTX_STATIC
+
+static u8 kfuz_mask[PAGE_SIZE];
+
+#elif HWIOFUZZ_MAPPING == HWIOFUZZ_PER_IRQ_DYNAMIC
+
+struct kfuz_access {
+	struct list_head list;
+	unsigned long addr;
+	size_t len;
+	int kfuz_area_offset;
+};
+
+// support only up to 1024 traps to cacheable memory in one interrupt
+#define HWIOFUZZ_ACCESS_POOL_SIZE 1024
+static struct kfuz_access kfuz_access_pool[HWIOFUZZ_ACCESS_POOL_SIZE];
+static atomic_t kfuz_access_pool_cnt;
+
+static LIST_HEAD(cacheable_writes);
+static LIST_HEAD(cacheable_reads);
+
+#endif
+
+#define FF(_b) (0xff << ((_b) << 3))
+
+static u32 count_bytes(u8 *mem, u32 mem_size)
+{
+	u32 *ptr = (u32 *)mem;
+	u32 i = (mem_size >> 2);
+	u32 ret = 0;
+
+	while (i--) {
+		u32 v = *(ptr++);
+
+		if (!v)
+			continue;
+		if (v & FF(0))
+			ret++;
+		if (v & FF(1))
+			ret++;
+		if (v & FF(2))
+			ret++;
+		if (v & FF(3))
+			ret++;
+	}
+	return ret;
+}
+
+bool kfuz_hwiotrace_init(void)
+{
+	struct kfuz *kfuz = hwiofuzz;
+
+#if HWIOFUZZ_MAPPING == HWIOFUZZ_PER_IRQ_DYNAMIC
+	struct kfuz_access *access;
+	struct kfuz_access *tmp;
+#endif
+
+	if (!kfuz || !kfuz->enabled || kfuz->inited || kfuz->consumed)
+		return false;
+
+	if (!kfuz_area)
+		return false;
+
+	pr_info("kfuz_hwiotrace_init kfuz_area=%p kfuz->size=%u count=%u\n",
+		kfuz_area, kfuz->size, count_bytes(kfuz_area, kfuz->size));
+
+	kfuz->inited = true;
+	kfuz->highwatermark = 0;
+
+#if HWIOFUZZ_MAPPING == HWIOFUZZ_PER_CTX_STATIC
+
+	memset(kfuz_mask, 0x0, kfuz->size);
+
+#elif HWIOFUZZ_MAPPING == HWIOFUZZ_PER_IRQ_DYNAMIC
+
+	atomic_set(&kfuz_access_pool_cnt, 0);
+
+	list_for_each_entry_safe (access, tmp, &cacheable_writes, list) {
+		list_del(&access->list);
+	}
+
+	list_for_each_entry_safe (access, tmp, &cacheable_reads, list) {
+		list_del(&access->list);
+	}
+
+#endif
+
+	return true;
+}
+
+bool kfuz_hwiotrace_exit(void)
+{
+	struct kfuz *kfuz = hwiofuzz;
+
+	if (!kfuz)
+		return false;
+
+	if (!kfuz_area)
+		return false;
+
+	if (!kfuz->inited)
+		return false;
+
+	pr_info("kfuz_hwiotrace_exit kfuz_area=%p kfuz->size=%u kfuz->highwatermark=0x%lx count=%u\n",
+		kfuz_area, kfuz->size, kfuz->highwatermark,
+		count_bytes(kfuz_area, kfuz->size));
+
+	kfuz->inited = false;
+
+	if (kfuz->highwatermark > 0) {
+		kfuz->consumed = true;
+		memset(kfuz_area, 0xab, kfuz->highwatermark);
+	}
+
+	return true;
+}
+
+bool kfuz_hwiotrace_consume(void *dest, unsigned long base, int offset,
+			    unsigned long len, bool cacheable)
+{
+	struct kfuz *kfuz = hwiofuzz;
+
+#if HWIOFUZZ_MAPPING == HWIOFUZZ_PER_CTX_STATIC
+
+	unsigned i;
+
+	if (offset < 0 || offset + len > PAGE_SIZE)
+		return false;
+
+#elif HWIOFUZZ_MAPPING == HWIOFUZZ_PER_IRQ_DYNAMIC
+
+	bool found = false;
+
+#endif
+
+	if (!dest)
+		return false;
+
+	if (!kfuz || !kfuz->inited || kfuz->consumed)
+		return false;
+
+	if (!kfuz_area)
+		return false;
+
+#if HWIOFUZZ_MAPPING == HWIOFUZZ_PER_CTX_STATIC
+
+	/*
+	 * highwatermark indicates the last offset to which the fuzzer buffer
+	 * has been consumed
+	 */
+
+	if (kfuz->highwatermark < offset + len)
+		kfuz->highwatermark = offset + len;
+
+	// TODO: is this too strict?
+	// if any byte of value is masked, do not fuzz
+	for (i = 0; i < len; i++) {
+		if (kfuz_mask[offset + i]) {
+			pr_info("kfuz_hwiotrace_consume masked at %d + %u\n",
+				offset, i);
+			return false;
+		}
+	}
+
+	memcpy(dest, kfuz_area + offset, len);
+
+#elif HWIOFUZZ_MAPPING == HWIOFUZZ_PER_IRQ_DYNAMIC
+
+	/*
+	 * highwatermark indicates next offset of the fuzzer buffer to consume
+	 */
+
+	if (kfuz->highwatermark + len > PAGE_SIZE)
+		return false;
+
+	if (cacheable) {
+		struct kfuz_access *access;
+		struct kfuz_access *tmp;
+		unsigned long addr = base + offset;
+
+		/*
+		 * we do not want to fuzz cacheable memory overwritten by driver
+		 */
+		list_for_each_entry (access, &cacheable_writes, list) {
+			// TODO: may need to be more precise
+			if ((access->addr <= addr &&
+			     addr < access->addr + access->len) ||
+			    (addr <= access->addr &&
+			     access->addr < addr + len)) {
+				pr_info("value overwritten by the driver\n");
+				return false;
+			}
+		}
+
+		/*
+		 * we do not want to provide different values to multi-reads
+		 * of cacheable memory
+		 */
+		list_for_each_entry_safe (access, tmp, &cacheable_reads, list) {
+			if (access->addr <= addr &&
+			    addr + len <= access->addr + access->len) {
+				memcpy(dest,
+				       kfuz_area + access->kfuz_area_offset,
+				       len);
+				pr_info("multi-reads of cacheable memory\n");
+				return true;
+			}
+
+			// handle partial overlap cases by overwriting fuzzer buffer
+			if ((addr <= access->addr &&
+			     access->addr < addr + len) ||
+			    (access->addr <= addr &&
+			     addr < access->addr + access->len)) {
+				int overlap_start = 0;
+				int overlap_end = len;
+				int kfuz_area_offset = 0;
+
+				if (addr < access->addr) {
+					overlap_start = access->addr - addr;
+				}
+
+				if (access->addr + access->len < addr + len) {
+					overlap_end = access->addr +
+						      access->len - addr;
+				}
+
+				if (access->addr < addr) {
+					kfuz_area_offset = addr - access->addr;
+				}
+
+				pr_info("overlapping multi-reads of cacheable memory at 0x%x (+0x%x)\n",
+					overlap_start,
+					overlap_end - overlap_start);
+
+				memcpy(kfuz_area + kfuz->highwatermark +
+					       overlap_start /* dest */,
+				       kfuz_area + access->kfuz_area_offset +
+					       kfuz_area_offset /* src */,
+				       overlap_end - overlap_start /* sz */);
+			}
+
+			// optimization
+			if (addr <= access->addr &&
+			    access->addr + len <= addr + len) {
+				pr_info("(optimization) updating previous cacheable read\n");
+				access->addr = addr;
+				access->len = len;
+				access->kfuz_area_offset = kfuz->highwatermark;
+				found = true;
+			}
+		}
+	}
+
+	memcpy(dest, kfuz_area + kfuz->highwatermark, len);
+
+	// add new entry for future cacheable reads
+	if (cacheable && !found) {
+		struct kfuz_access *new_access;
+		int idx = atomic_inc_return(&kfuz_access_pool_cnt);
+		if (idx > HWIOFUZZ_ACCESS_POOL_SIZE) {
+			pr_warn("kfuz_hwiotrace_consume pool alloc failed\n");
+			return false;
+		}
+		new_access = &kfuz_access_pool[idx - 1];
+		new_access->addr = base + offset;
+		new_access->len = len;
+		new_access->kfuz_area_offset = kfuz->highwatermark;
+
+		list_add(&new_access->list, &cacheable_reads);
+	}
+
+	kfuz->highwatermark += len;
+	pr_info("new highwatermark=0x%lx\n", kfuz->highwatermark);
+
+#endif
+
+	return true;
+}
+
+bool kfuz_hwiotrace_mask(unsigned long base, int offset, unsigned long len)
+{
+	struct kfuz *kfuz = hwiofuzz;
+
+#if HWIOFUZZ_MAPPING == HWIOFUZZ_PER_IRQ_DYNAMIC
+	struct kfuz_access *access;
+	bool found = false;
+#endif
+
+	if (!kfuz || !kfuz->inited || kfuz->consumed)
+		return false;
+
+#if HWIOFUZZ_MAPPING == HWIOFUZZ_PER_CTX_STATIC
+
+	if (offset < 0 || offset + len > PAGE_SIZE)
+		return false;
+
+	// TODO: should we update high-water mark?
+
+	memset(kfuz_mask + offset, 0xff, len);
+
+#elif HWIOFUZZ_MAPPING == HWIOFUZZ_PER_IRQ_DYNAMIC
+
+	list_for_each_entry (access, &cacheable_writes, list) {
+		unsigned long addr = base + offset;
+
+		if (addr <= access->addr && access->addr <= addr + len) {
+			access->addr = addr;
+			if (access->addr + access->len < addr + len) {
+				access->len = addr + len - access->addr;
+			}
+			found = true;
+			break;
+		}
+
+		if (access->addr <= addr &&
+		    addr <= access->addr + access->len) {
+			if (access->addr + access->len < addr + len) {
+				access->len = addr + len - access->addr;
+			}
+			found = true;
+			break;
+		}
+	}
+
+	if (!found) {
+		struct kfuz_access *new_access;
+		int idx = atomic_inc_return(&kfuz_access_pool_cnt);
+		if (idx > HWIOFUZZ_ACCESS_POOL_SIZE) {
+			pr_warn("kfuz_hwiotrace_consume pool alloc failed\n");
+			return false;
+		}
+		new_access = &kfuz_access_pool[idx - 1];
+		new_access->addr = base + offset;
+		new_access->len = len;
+
+		list_add(&new_access->list, &cacheable_writes);
+	}
+
+#endif
+
+	return true;
+}
+
+static void kfuz_get(struct kfuz *kfuz)
+{
+	atomic_inc(&kfuz->refcount);
+}
+
+static void kfuz_put(struct kfuz *kfuz)
+{
+	if (atomic_dec_and_test(&kfuz->refcount)) {
+		hwiofuzz = NULL;
+		kfuz_area = NULL;
+		vfree(kfuz->area);
+		kfree(kfuz);
+	}
+}
+
+static int kfuz_mmap(struct file *filep, struct vm_area_struct *vma)
+{
+	int res = 0;
+	void *area;
+	struct kfuz *kfuz = vma->vm_file->private_data;
+	unsigned long size, off;
+	struct page *page;
+
+	area = vmalloc_user(vma->vm_end - vma->vm_start);
+	if (!area)
+		return -ENOMEM;
+
+	spin_lock(&kfuz->lock);
+	size = kfuz->size;
+
+	if (kfuz->mode == HWIOFUZZ_MODE_DISABLED || vma->vm_pgoff != 0 ||
+	    vma->vm_end - vma->vm_start != size) {
+		res = -EINVAL;
+		goto exit;
+	}
+	if (!kfuz->area) {
+		kfuz->area = area;
+		vma->vm_flags |= VM_DONTEXPAND;
+		spin_unlock(&kfuz->lock);
+		for (off = 0; off < size; off += PAGE_SIZE) {
+			page = vmalloc_to_page(kfuz->area + off);
+			if (vm_insert_page(vma, vma->vm_start + off, page))
+				WARN_ONCE(1, "vm_insert_page() failed");
+		}
+		return 0;
+	}
+exit:
+	spin_unlock(&kfuz->lock);
+	vfree(area);
+	return res;
+}
+
+static int kfuz_open(struct inode *inode, struct file *filep)
+{
+	struct kfuz *kfuz;
+
+	kfuz = kzalloc(sizeof(*kfuz), GFP_KERNEL);
+	if (!kfuz)
+		return -ENOMEM;
+	atomic_set(&kfuz->refcount, 1);
+	spin_lock_init(&kfuz->lock);
+	filep->private_data = kfuz;
+	return nonseekable_open(inode, filep);
+}
+
+static int kfuz_close(struct inode *inode, struct file *filep)
+{
+	kfuz_put(filep->private_data);
+	return 0;
+}
+
+static long kfuz_ioctl_locked(struct kfuz *kfuz, unsigned int cmd,
+			      unsigned long arg)
+{
+	struct task_struct *t;
+	unsigned long size, unused;
+
+	switch (cmd) {
+	case HWIOFUZZ_INIT:
+		if (kfuz->mode != HWIOFUZZ_MODE_DISABLED)
+			return -EBUSY;
+		size = arg;
+		if (size < 1 || size > PAGE_SIZE)
+			return -EINVAL;
+		kfuz->size = size;
+		kfuz->mode = HWIOFUZZ_MODE_TRACE;
+		return 0;
+	case HWIOFUZZ_CONSUME_ENABLE:
+		unused = arg;
+		if (unused != 0 || kfuz->mode == HWIOFUZZ_MODE_DISABLED ||
+		    kfuz->area == NULL)
+			return -EINVAL;
+		pr_info("consumption enabled\n");
+		t = current;
+		kfuz->t = t;
+		kfuz->enabled = true;
+		kfuz->inited = false;
+		kfuz->consumed = false;
+		kfuz_get(kfuz);
+		hwiofuzz = kfuz;
+		kfuz_area = kfuz->area;
+		return 0;
+	case HWIOFUZZ_CONSUME_CHECK:
+		pr_info("highwatermark=0x%lx\n", kfuz->highwatermark);
+		return kfuz->consumed ? kfuz->highwatermark : -EINVAL;
+	case HWIOFUZZ_CONSUME_DISABLE:
+		pr_info("consumption disabled\n");
+		t = current;
+		kfuz->t = NULL;
+		kfuz->enabled = false;
+		hwiofuzz = NULL;
+		kfuz_area = NULL;
+		kfuz_put(kfuz);
+		return 0;
+	default:
+		return -ENOTTY;
+	}
+}
+
+static long kfuz_ioctl(struct file *filep, unsigned int cmd, unsigned long arg)
+{
+	struct kfuz *kfuz;
+	int res;
+
+	kfuz = filep->private_data;
+	spin_lock(&kfuz->lock);
+	res = kfuz_ioctl_locked(kfuz, cmd, arg);
+	spin_unlock(&kfuz->lock);
+	return res;
+}
+
+static const struct file_operations kfuz_fops = {
+	.open = kfuz_open,
+	.unlocked_ioctl = kfuz_ioctl,
+	.mmap = kfuz_mmap,
+	.release = kfuz_close,
+};
+
+static int __init hwiofuzz_init(void)
+{
+	if (!debugfs_create_file("kfuz", 0600, NULL, NULL, &kfuz_fops)) {
+		pr_err("failed to create kfuz in debugfs\n");
+		return -ENOMEM;
+	}
+	return 0;
+}
+
+device_initcall(hwiofuzz_init);
diff -ruN linux-5.15.0/kernel/trace/trace_hwiotrace.c linux-5.15.0-patched/kernel/trace/trace_hwiotrace.c
--- linux-5.15.0/kernel/trace/trace_hwiotrace.c	1969-12-31 19:00:00.000000000 -0500
+++ linux-5.15.0-patched/kernel/trace/trace_hwiotrace.c	2025-09-17 11:55:25.424137380 -0400
@@ -0,0 +1,781 @@
+/*
+ * HW I/O tracing
+ */
+
+#define DEBUG 1
+
+#include <linux/kernel.h>
+#include <linux/debugfs.h>
+#include <linux/pci.h>
+#include <linux/slab.h>
+#include <linux/time.h>
+
+#include <linux/atomic.h>
+
+#include "trace.h"
+#include "trace_output.h"
+
+struct header_iter {
+	struct pci_dev *dev;
+};
+
+#define TRACE_HWIO_WLAN 0x1
+
+#define TRACE_HWIO_MMIO 0x10
+
+#define TRACE_HWIO_DMA_SYNC 0x100
+#define TRACE_HWIO_DMA_MAP 0x200
+
+static struct tracer_opt trace_opts[] = {
+	/* Trace MMIO */
+	{ TRACER_OPT(hwio - mmio, TRACE_HWIO_MMIO) },
+	/* Trace DMA */
+	{ TRACER_OPT(hwio - dma - sync, TRACE_HWIO_DMA_SYNC) },
+	{ TRACER_OPT(hwio - dma - map, TRACE_HWIO_DMA_MAP) },
+	/* Trace WLAN ? */
+	{ TRACER_OPT(hwio - wlan, TRACE_HWIO_WLAN) },
+	{} /* Empty entry */
+};
+
+static struct tracer_flags tracer_flags = {
+	/* Don't trace anything by default */
+	.val = 0x0,
+	.opts = trace_opts
+};
+
+static struct trace_array *hwio_trace_array;
+static bool overrun_detected;
+static unsigned long prev_overruns;
+static atomic_t dropped_count;
+static int ftrace_hwio_dma_sync;
+static int ftrace_hwio_dma_map;
+static int ftrace_hwio_mmio;
+
+static void hwio_reset_data(struct trace_array *tr)
+{
+	overrun_detected = false;
+	prev_overruns = 0;
+
+	tracing_reset_online_cpus(&tr->trace_buffer);
+}
+
+static void hwio_trace_start(struct trace_array *tr)
+{
+	pr_debug("in %s\n", __func__);
+	hwio_reset_data(tr);
+}
+
+static void hwio_print_pcidev(struct trace_seq *s, const struct pci_dev *dev)
+{
+	int i;
+	resource_size_t start, end;
+	const struct pci_driver *drv = pci_dev_driver(dev);
+	struct dma_sync_trace *trace;
+	struct dma_sync_trace *tmp;
+	struct list_head *trace_list;
+	unsigned long flags;
+	unsigned long long t = 0; //ns2usecs(iter->ts);
+	unsigned long usec_rem = do_div(t, USEC_PER_SEC);
+	unsigned secs = (unsigned long)t;
+
+	trace_seq_printf(s, "PCIDEV %02x%02x %04x%04x %x", dev->bus->number,
+			 dev->devfn, dev->vendor, dev->device, dev->irq);
+
+	// print all DMA buffers currently mapped
+	trace_list = sync_trace_list_get(&flags);
+	list_for_each_entry_safe (trace, tmp, trace_list, list) {
+		trace_seq_printf(
+			s,
+			"MAP %u.%06lu %ld 0x%llx 0x%lx 0x%lx 0x%lx 0x%lx %d\n",
+			secs, usec_rem, trace->id,
+			(unsigned long long)trace->phys, trace->probe.addr,
+			trace->probe.len, trace->probe.ctx, 0UL, 0);
+	}
+	sync_trace_list_put(flags);
+
+	/*
+	 * XXX: is pci_resource_to_user() appropriate, since we are
+	 * supposed to interpret the __ioremap() phys_addr argument based on
+	 * these printed values?
+	 */
+	for (i = 0; i < 7; i++) {
+		pci_resource_to_user(dev, i, &dev->resource[i], &start, &end);
+		trace_seq_printf(s, " %llx",
+				 (unsigned long long)(start |
+						      (dev->resource[i].flags &
+						       PCI_REGION_FLAG_MASK)));
+	}
+	for (i = 0; i < 7; i++) {
+		pci_resource_to_user(dev, i, &dev->resource[i], &start, &end);
+		trace_seq_printf(s, " %llx",
+				 dev->resource[i].start < dev->resource[i].end ?
+					 (unsigned long long)(end - start) + 1 :
+					 0);
+	}
+	if (drv)
+		trace_seq_printf(s, " %s\n", drv->name);
+	else
+		trace_seq_puts(s, " \n");
+}
+
+static void destroy_header_iter(struct header_iter *hiter)
+{
+	if (!hiter)
+		return;
+	pci_dev_put(hiter->dev);
+	kfree(hiter);
+}
+
+static void hwio_pipe_open(struct trace_iterator *iter)
+{
+	struct header_iter *hiter;
+	struct trace_seq *s = &iter->seq;
+
+	trace_seq_puts(s, "VERSION 20180114\n");
+
+	hiter = kzalloc(sizeof(*hiter), GFP_KERNEL);
+	if (!hiter)
+		return;
+
+	hiter->dev = pci_get_device(PCI_ANY_ID, PCI_ANY_ID, NULL);
+	iter->private = hiter;
+}
+
+/* XXX: This is not called when the pipe is closed! */
+static void hwio_close(struct trace_iterator *iter)
+{
+	struct header_iter *hiter = iter->private;
+	destroy_header_iter(hiter);
+	iter->private = NULL;
+}
+
+static unsigned long count_overruns(struct trace_iterator *iter)
+{
+	unsigned long cnt = atomic_xchg(&dropped_count, 0);
+	unsigned long over = ring_buffer_overruns(iter->trace_buffer->buffer);
+
+	if (over > prev_overruns)
+		cnt += over - prev_overruns;
+	prev_overruns = over;
+	return cnt;
+}
+
+static ssize_t hwio_read(struct trace_iterator *iter, struct file *filp,
+			 char __user *ubuf, size_t cnt, loff_t *ppos)
+{
+	ssize_t ret;
+	struct header_iter *hiter = iter->private;
+	struct trace_seq *s = &iter->seq;
+	unsigned long n;
+
+	n = count_overruns(iter);
+	if (n) {
+		/* XXX: This is later than where events were lost. */
+		trace_seq_printf(s, "MARK 0.000000 Lost %lu events.\n", n);
+		if (!overrun_detected)
+			pr_warning("hwiotrace has lost events.\n");
+		overrun_detected = true;
+		goto print_out;
+	}
+
+	if (!hiter)
+		return 0;
+
+	hwio_print_pcidev(s, hiter->dev);
+	hiter->dev = pci_get_device(PCI_ANY_ID, PCI_ANY_ID, hiter->dev);
+
+	if (!hiter->dev) {
+		destroy_header_iter(hiter);
+		iter->private = NULL;
+	}
+
+print_out:
+	ret = trace_seq_to_user(s, ubuf, cnt);
+	return (ret == -EBUSY) ? 0 : ret;
+}
+
+static enum print_line_t hwio_print_rw(struct trace_iterator *iter)
+{
+	struct trace_entry *entry = iter->ent;
+	struct trace_hwiotrace_rw *field;
+	struct hwiotrace_rw *rw;
+	struct trace_seq *s = &iter->seq;
+	unsigned long long t = ns2usecs(iter->ts);
+	unsigned long usec_rem = do_div(t, USEC_PER_SEC);
+	unsigned secs = (unsigned long)t;
+
+	trace_assign_type(field, entry);
+	rw = &field->rw;
+
+	switch (rw->opcode) {
+	case HWIO_READ:
+		trace_seq_printf(
+			s, "R %d %u.%06lu %d 0x%llx 0x%lx 0x%lx 0x%lx %d\n",
+			rw->width, secs, usec_rem, rw->map_id,
+			(unsigned long long)rw->phys, rw->virt, rw->value,
+			rw->pc, 0);
+		break;
+	case HWIO_WRITE:
+		trace_seq_printf(
+			s, "W %d %u.%06lu %d 0x%llx 0x%lx 0x%lx 0x%lx %d\n",
+			rw->width, secs, usec_rem, rw->map_id,
+			(unsigned long long)rw->phys, rw->virt, rw->value,
+			rw->pc, 0);
+		break;
+	case HWIO_UNKNOWN_OP:
+		trace_seq_printf(s,
+				 "UNKNOWN %u.%06lu %d 0x%llx %02lx,%02lx,"
+				 "%02lx 0x%lx %d\n",
+				 secs, usec_rem, rw->map_id,
+				 (unsigned long long)rw->phys,
+				 (rw->value >> 16) & 0xff,
+				 (rw->value >> 8) & 0xff,
+				 (rw->value >> 0) & 0xff, rw->pc, 0);
+		break;
+	default:
+		trace_seq_puts(s, "rw what?\n");
+		break;
+	}
+
+	return trace_handle_return(s);
+}
+
+static enum print_line_t hwio_print_map(struct trace_iterator *iter)
+{
+	struct trace_entry *entry = iter->ent;
+	struct trace_hwiotrace_map *field;
+	struct hwiotrace_map *m;
+	struct trace_seq *s = &iter->seq;
+	unsigned long long t = ns2usecs(iter->ts);
+	unsigned long usec_rem = do_div(t, USEC_PER_SEC);
+	unsigned secs = (unsigned long)t;
+
+	trace_assign_type(field, entry);
+	m = &field->map;
+
+	switch (m->opcode) {
+	case HWIO_PROBE:
+		trace_seq_printf(
+			s,
+			"MAP %u.%06lu %d 0x%llx 0x%lx 0x%lx 0x%lx 0x%lx %d\n",
+			secs, usec_rem, m->map_id, (unsigned long long)m->phys,
+			m->virt, m->len, m->pc, 0UL, 0);
+		break;
+	case HWIO_UNPROBE:
+		trace_seq_printf(s, "UNMAP %u.%06lu %d 0x%lx %d\n", secs,
+				 usec_rem, m->map_id, 0UL, 0);
+		break;
+	default:
+		trace_seq_puts(s, "map what?\n");
+		break;
+	}
+
+	return trace_handle_return(s);
+}
+
+static enum print_line_t hwio_print_task(struct trace_iterator *iter)
+{
+	struct trace_entry *entry = iter->ent;
+	struct trace_hwiotrace_task *field;
+	struct hwiotrace_task *tsk;
+	struct trace_seq *s = &iter->seq;
+	unsigned long long t = ns2usecs(iter->ts);
+	unsigned long usec_rem = do_div(t, USEC_PER_SEC);
+	unsigned secs = (unsigned long)t;
+
+	trace_assign_type(field, entry);
+	tsk = &field->task;
+
+	switch (tsk->opcode) {
+	case HWIO_TASK_START:
+		trace_seq_printf(s, "START %u.%06lu %d\n", secs, usec_rem,
+				 tsk->task_id);
+		break;
+	case HWIO_TASK_END:
+		trace_seq_printf(s, "END %u.%06lu %d\n", secs, usec_rem,
+				 tsk->task_id);
+		break;
+	default:
+		trace_seq_puts(s, "task what?\n");
+		break;
+	}
+
+	return trace_handle_return(s);
+}
+
+static enum print_line_t hwio_print_mark(struct trace_iterator *iter)
+{
+	struct trace_entry *entry = iter->ent;
+	struct print_entry *print = (struct print_entry *)entry;
+	const char *msg = print->buf;
+	struct trace_seq *s = &iter->seq;
+	unsigned long long t = ns2usecs(iter->ts);
+	unsigned long usec_rem = do_div(t, USEC_PER_SEC);
+	unsigned secs = (unsigned long)t;
+
+	/* The trailing newline must be in the message. */
+	trace_seq_printf(s, "MARK %u.%06lu %s", secs, usec_rem, msg);
+
+	return trace_handle_return(s);
+}
+
+static enum print_line_t hwio_print_line(struct trace_iterator *iter)
+{
+	switch (iter->ent->type) {
+	case TRACE_HWIO_RW:
+		return hwio_print_rw(iter);
+	case TRACE_HWIO_MAP:
+		return hwio_print_map(iter);
+	case TRACE_HWIO_TASK:
+		return hwio_print_task(iter);
+	case TRACE_PRINT:
+		return hwio_print_mark(iter);
+	default:
+		return TRACE_TYPE_HANDLED; /* ignore unknown entries */
+	}
+}
+
+// reset tracing for current dev
+static void hwio_trace_reset_dev(void)
+{
+	disable_hwiotrace();
+}
+
+static void hwio_trace_init_dev(char *dev)
+{
+	hwio_trace_reset_dev();
+
+	activate_hwiotrace(dev, ftrace_hwio_dma_sync, ftrace_hwio_dma_map,
+			   ftrace_hwio_mmio);
+}
+
+#define WLAN_DRIVER_NAME "icnss"
+
+static void ftrace_hwio_wlan(int set)
+{
+	if (set) {
+		hwio_trace_init_dev(WLAN_DRIVER_NAME);
+	} else {
+		hwio_trace_reset_dev();
+	}
+}
+
+static int hwio_set_flag(struct trace_array *tr, u32 old_flags, u32 bit,
+			 int set)
+{
+	if (bit == TRACE_HWIO_DMA_SYNC)
+		ftrace_hwio_dma_sync = set;
+
+	if (bit == TRACE_HWIO_DMA_MAP)
+		ftrace_hwio_dma_map = set;
+
+	if (bit == TRACE_HWIO_MMIO)
+		ftrace_hwio_mmio = set;
+
+	if (bit == TRACE_HWIO_WLAN)
+		ftrace_hwio_wlan(set);
+
+	return 0;
+}
+
+static void __trace_hwiotrace_rw(struct trace_array *tr,
+				 struct trace_array_cpu *data,
+				 struct hwiotrace_rw *rw)
+{
+	struct trace_event_call *call = &event_hwiotrace_rw;
+	struct ring_buffer *buffer = tr->trace_buffer.buffer;
+	struct ring_buffer_event *event;
+	struct trace_hwiotrace_rw *entry;
+	int pc = preempt_count();
+
+	event = trace_buffer_lock_reserve(buffer, TRACE_HWIO_RW, sizeof(*entry),
+					  0, pc);
+	if (!event) {
+		atomic_inc(&dropped_count);
+		return;
+	}
+	entry = ring_buffer_event_data(event);
+	entry->rw = *rw;
+
+	if (!call_filter_check_discard(call, entry, buffer, event))
+		trace_buffer_unlock_commit(tr, buffer, event, 0, pc);
+}
+
+void hwio_trace_rw(struct hwiotrace_rw *rw)
+{
+	struct trace_array *tr = hwio_trace_array;
+	struct trace_array_cpu *data =
+		per_cpu_ptr(tr->trace_buffer.data, smp_processor_id());
+	__trace_hwiotrace_rw(tr, data, rw);
+}
+
+static void __trace_hwiotrace_map(struct trace_array *tr,
+				  struct trace_array_cpu *data,
+				  struct hwiotrace_map *map)
+{
+	struct trace_event_call *call = &event_hwiotrace_map;
+	struct ring_buffer *buffer = tr->trace_buffer.buffer;
+	struct ring_buffer_event *event;
+	struct trace_hwiotrace_map *entry;
+	int pc = preempt_count();
+
+	event = trace_buffer_lock_reserve(buffer, TRACE_HWIO_MAP,
+					  sizeof(*entry), 0, pc);
+	if (!event) {
+		atomic_inc(&dropped_count);
+		return;
+	}
+	entry = ring_buffer_event_data(event);
+	entry->map = *map;
+
+	if (!call_filter_check_discard(call, entry, buffer, event))
+		trace_buffer_unlock_commit(tr, buffer, event, 0, pc);
+}
+
+void hwio_trace_mapping(struct hwiotrace_map *map)
+{
+	struct trace_array *tr = hwio_trace_array;
+	struct trace_array_cpu *data;
+
+	preempt_disable();
+	data = per_cpu_ptr(tr->trace_buffer.data, smp_processor_id());
+	__trace_hwiotrace_map(tr, data, map);
+	preempt_enable();
+}
+
+static void __trace_hwiotrace_task(struct trace_array *tr,
+				   struct trace_array_cpu *data,
+				   struct hwiotrace_task *task)
+{
+	struct trace_event_call *call = &event_hwiotrace_task;
+	struct ring_buffer *buffer = tr->trace_buffer.buffer;
+	struct ring_buffer_event *event;
+	struct trace_hwiotrace_task *entry;
+	int pc = preempt_count();
+
+	event = trace_buffer_lock_reserve(buffer, TRACE_HWIO_TASK,
+					  sizeof(*entry), 0, pc);
+	if (!event) {
+		atomic_inc(&dropped_count);
+		return;
+	}
+	entry = ring_buffer_event_data(event);
+	entry->task = *task;
+
+	if (!call_filter_check_discard(call, entry, buffer, event))
+		trace_buffer_unlock_commit(tr, buffer, event, 0, pc);
+}
+
+void hwio_trace_task(struct hwiotrace_task *task)
+{
+	struct trace_array *tr = hwio_trace_array;
+	struct trace_array_cpu *data;
+
+	preempt_disable();
+	data = per_cpu_ptr(tr->trace_buffer.data, smp_processor_id());
+	__trace_hwiotrace_task(tr, data, task);
+	preempt_enable();
+}
+
+int hwio_trace_printk(const char *fmt, va_list args)
+{
+	return trace_vprintk(0, fmt, args);
+}
+
+static struct trace_parser drv_set_parser;
+static struct trace_parser drv_unset_parser;
+static struct trace_parser id_set_parser;
+static struct trace_parser id_unset_parser;
+static struct trace_parser ctx_set_parser;
+static struct trace_parser ctx_unset_parser;
+
+/////////////////////////////////////////////////////////////////////////////////////
+// SHOW ACTIVE MAPPINGS /////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////
+static int mappings_avail_seq_show(struct seq_file *s, void *v)
+{
+	struct list_head *mappings;
+	struct hwiotrace_mapping *am;
+	unsigned long flags;
+
+	seq_printf(s, "ID NAME CTX VA SIZE STATE\n");
+	mappings = hwiotrace_mappings_get(&flags);
+	list_for_each_entry (am, mappings, list_glob) {
+		seq_printf(s, "%zu %s 0x%lx %lx %ld %s\n", am->map_id,
+			   am->pp->drv_name, am->pp->ctx, (unsigned long)am->va,
+			   am->size, am->tracing ? "TRACING" : "-");
+	}
+	hwiotrace_mappings_put(flags);
+	return 0;
+}
+static int mappings_avail_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, mappings_avail_seq_show, 0);
+}
+static const struct file_operations mappings_avail_fops = {
+	.open = mappings_avail_open,
+	.read = seq_read,
+	.release = single_release,
+};
+
+/////////////////////////////////////////////////////////////////////////////////////
+// SHOW AVAILABLE PROBES ////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////
+static int probes_avail_seq_show(struct seq_file *s, void *v)
+{
+	struct list_head *probes;
+	struct hwiotrace_probe *p;
+	unsigned long flags;
+
+	seq_printf(s, "ID NAME CTX TYPE SIZE STATE\n");
+	probes = hwiotrace_probes_get(&flags);
+	list_for_each_entry (p, probes, list) {
+		seq_printf(s, "%zu %s 0x%lx %s %ld %s\n", p->id, p->drv_name,
+			   p->ctx, probe_type_names[p->type], p->last_length,
+			   p->tracing ? "TRACING" : "-");
+	}
+	hwiotrace_probes_put(flags);
+	return 0;
+}
+static int probes_avail_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, probes_avail_seq_show, 0);
+}
+static const struct file_operations probes_avail_fops = {
+	.open = probes_avail_open,
+	.read = seq_read,
+	.release = single_release,
+};
+
+static int filter_open(struct inode *inode, struct file *file)
+{
+	return 0;
+}
+
+/////////////////////////////////////////////////////////////////////////////////////
+// ID FILTER ////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////
+ssize_t id_set_filter_write(struct file *file, const char __user *ubuf,
+			    size_t cnt, loff_t *ppos)
+{
+	size_t read, id;
+	read = trace_get_user(&id_set_parser, ubuf, cnt, ppos);
+	if (read >= 0 && trace_parser_loaded(&id_set_parser) &&
+	    !trace_parser_cont(&id_set_parser)) {
+		if (kstrtoul(id_set_parser.buffer, 10, &id) == 0)
+			activate_hwiotrace_probe(NULL, 0, id);
+	}
+	return read;
+}
+ssize_t id_unset_filter_write(struct file *file, const char __user *ubuf,
+			      size_t cnt, loff_t *ppos)
+{
+	size_t read, id;
+	read = trace_get_user(&id_unset_parser, ubuf, cnt, ppos);
+	if (read >= 0 && trace_parser_loaded(&id_unset_parser) &&
+	    !trace_parser_cont(&id_unset_parser)) {
+		if (kstrtoul(id_unset_parser.buffer, 10, &id) == 0)
+			deactivate_hwiotrace_probe(NULL, 0, id);
+	}
+	return read;
+}
+static const struct file_operations id_set_filter_fops = {
+	.open = filter_open,
+	.read = seq_read,
+	.write = id_set_filter_write,
+};
+static const struct file_operations id_unset_filter_fops = {
+	.open = filter_open,
+	.read = seq_read,
+	.write = id_unset_filter_write,
+};
+
+/////////////////////////////////////////////////////////////////////////////////////
+// CTX FILTER ///////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////
+ssize_t ctx_set_filter_write(struct file *file, const char __user *ubuf,
+			     size_t cnt, loff_t *ppos)
+{
+	size_t read;
+	unsigned long ctx;
+	read = trace_get_user(&ctx_set_parser, ubuf, cnt, ppos);
+	if (read >= 0 && trace_parser_loaded(&ctx_set_parser) &&
+	    !trace_parser_cont(&ctx_set_parser)) {
+		if (kstrtoul(ctx_set_parser.buffer, 16, &ctx) == 0)
+			activate_hwiotrace_probe(NULL, ctx, 0);
+	}
+	return read;
+}
+ssize_t ctx_unset_filter_write(struct file *file, const char __user *ubuf,
+			       size_t cnt, loff_t *ppos)
+{
+	size_t read;
+	unsigned long ctx;
+	read = trace_get_user(&ctx_unset_parser, ubuf, cnt, ppos);
+	if (read >= 0 && trace_parser_loaded(&ctx_unset_parser) &&
+	    !trace_parser_cont(&ctx_unset_parser)) {
+		if (kstrtoul(ctx_unset_parser.buffer, 16, &ctx) == 0)
+			deactivate_hwiotrace_probe(NULL, ctx, 0);
+	}
+	return read;
+}
+static const struct file_operations ctx_set_filter_fops = {
+	.open = filter_open,
+	.read = seq_read,
+	.write = ctx_set_filter_write,
+};
+static const struct file_operations ctx_unset_filter_fops = {
+	.open = filter_open,
+	.read = seq_read,
+	.write = ctx_unset_filter_write,
+};
+
+/////////////////////////////////////////////////////////////////////////////////////
+// DRV FILTER ///////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////
+ssize_t drv_set_filter_write(struct file *file, const char __user *ubuf,
+			     size_t cnt, loff_t *ppos)
+{
+	size_t read;
+	read = trace_get_user(&drv_set_parser, ubuf, cnt, ppos);
+	if (read >= 0 && trace_parser_loaded(&drv_set_parser) &&
+	    !trace_parser_cont(&drv_set_parser)) {
+		activate_hwiotrace_probe(drv_set_parser.buffer, 0, 0);
+	}
+	return read;
+}
+ssize_t drv_unset_filter_write(struct file *file, const char __user *ubuf,
+			       size_t cnt, loff_t *ppos)
+{
+	size_t read;
+	read = trace_get_user(&drv_unset_parser, ubuf, cnt, ppos);
+	if (read >= 0 && trace_parser_loaded(&drv_unset_parser) &&
+	    !trace_parser_cont(&drv_unset_parser)) {
+		deactivate_hwiotrace_probe(drv_set_parser.buffer, 0, 0);
+	}
+	return read;
+}
+static const struct file_operations drv_set_filter_fops = {
+	.open = filter_open,
+	.read = seq_read,
+	.write = drv_set_filter_write,
+};
+static const struct file_operations drv_unset_filter_fops = {
+	.open = filter_open,
+	.read = seq_read,
+	.write = drv_unset_filter_write,
+};
+
+/////////////////////////////////////////////////////////////////////////////////////
+// INITALIZER ///////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////
+
+#define MAX_FILTER_SIZE 1024
+
+static int init_tracer_fs(struct trace_array *tr)
+{
+	struct dentry *t_options = tr->options; //trace_options_init_dentry(tr);
+	if (!t_options) {
+		pr_err("Error init_tracer_fs no options directory\n");
+	}
+
+	debugfs_create_file("hwio_available_mappings", 0444, t_options, tr,
+			    &mappings_avail_fops);
+
+	debugfs_create_file("hwio_available_probes", 0444, t_options, tr,
+			    &probes_avail_fops);
+
+	debugfs_create_file("hwio_drv_set_filter", 0644, t_options, tr,
+			    &drv_set_filter_fops);
+	debugfs_create_file("hwio_drv_unset_filter", 0644, t_options, tr,
+			    &drv_unset_filter_fops);
+
+	debugfs_create_file("hwio_id_set_filter", 0644, t_options, tr,
+			    &id_set_filter_fops);
+	debugfs_create_file("hwio_id_unset_filter", 0644, t_options, tr,
+			    &id_unset_filter_fops);
+
+	debugfs_create_file("hwio_ctx_set_filter", 0644, t_options, tr,
+			    &ctx_set_filter_fops);
+	debugfs_create_file("hwio_ctx_unset_filter", 0644, t_options, tr,
+			    &ctx_unset_filter_fops);
+
+	trace_parser_get_init(&drv_set_parser, MAX_FILTER_SIZE);
+	trace_parser_get_init(&drv_unset_parser, MAX_FILTER_SIZE);
+
+	trace_parser_get_init(&id_set_parser, MAX_FILTER_SIZE);
+	trace_parser_get_init(&id_unset_parser, MAX_FILTER_SIZE);
+
+	trace_parser_get_init(&ctx_set_parser, MAX_FILTER_SIZE);
+	trace_parser_get_init(&ctx_unset_parser, MAX_FILTER_SIZE);
+
+	return 0;
+}
+
+// TODO: should remove debugfs option files here but don't know how
+static int reset_tracer_fs(struct trace_array *tr)
+{
+	stop_all_hwiotrace_probes();
+	return 0;
+}
+
+static int hwio_trace_init(struct trace_array *tr)
+{
+	pr_debug("in %s\n", __func__);
+	hwio_trace_array = tr;
+
+	hwio_reset_data(tr);
+	enable_hwiotrace();
+	init_tracer_fs(hwio_trace_array);
+	return 0;
+}
+
+static void hwio_trace_reset(struct trace_array *tr)
+{
+	pr_debug("in %s\n", __func__);
+
+	reset_tracer_fs(tr);
+	hwio_reset_data(tr);
+	hwio_trace_array = NULL;
+}
+
+/**
+ * struct tracer - a specific tracer and its callbacks to interact with tracefs
+ * @name: the name chosen to select it on the available_tracers file
+ * @init: called when one switches to this tracer (echo name > current_tracer)
+ * @reset: called when one switches to another tracer
+ * @start: called when tracing is unpaused (echo 1 > tracing_enabled)
+ * @stop: called when tracing is paused (echo 0 > tracing_enabled)
+ * @update_thresh: called when tracing_thresh is updated
+ * @open: called when the trace file is opened
+ * @pipe_open: called when the trace_pipe file is opened
+ * @close: called when the trace file is released
+ * @pipe_close: called when the trace_pipe file is released
+ * @read: override the default read callback on trace_pipe
+ * @splice_read: override the default splice_read callback on trace_pipe
+ * @selftest: selftest to run on boot (see trace_selftest.c)
+ * @print_headers: override the first lines that describe your columns
+ * @print_line: callback that prints a trace
+ * @set_flag: signals one of your private flags changed (trace_options file)
+ * @flags: your private flags
+ */
+
+static struct tracer hwio_tracer __read_mostly = {
+	.name = "hwiotrace",
+	.init = hwio_trace_init,
+	.reset = hwio_trace_reset,
+	.start = hwio_trace_start,
+	.pipe_open = hwio_pipe_open,
+	.close = hwio_close,
+	.read = hwio_read,
+	.print_line = hwio_print_line,
+	.flags = &tracer_flags,
+	.set_flag = hwio_set_flag,
+};
+
+__init static int init_hwio_trace(void)
+{
+	return register_tracer(&hwio_tracer);
+}
+
+device_initcall(init_hwio_trace);
